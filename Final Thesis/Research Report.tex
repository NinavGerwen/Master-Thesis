% setting document class according to SAGE guidelines
\documentclass[Royal,sageapa,times,doublespace]{sagej}

% packages for APA style requirements according to SAGE guidelines
\usepackage{moreverb,url}
\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}

% mathematical (equation) packages
\usepackage{amsmath}
\usepackage{amssymb}

% table-related packages for APA style tables
\usepackage{multirow,booktabs,setspace,caption}
\usepackage{tikz}
\usepackage{xcolor, colortbl}

% some document-wide table settings to be able to use
\definecolor{Gray}{gray}{0.90}
\newcolumntype{a}{>{\columncolor{Gray}}c}

\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\def\volumeyear{2023}


\begin{document}

\runninghead{van Gerwen and Hessen}

\title{Assessing Fit of Item Response Theory Models using a Likelihood Ratio Randomisation Test and Fit Indices}

\author{Nina van Gerwen \affilnum{1} and Dave Hessen\affilnum{2}}

\affiliation{\affilnum{1}Utrecht University, NL \\
\affilnum{2}Utrecht University, NL}

\corrauth{Nina van Gerwen, Utrecht University,
Faculty of Social Sciences, Department of Methodology and Statistics,
Padualaan 14, Utrecht, 3584 CH, NL.}

\email{n.l.vangerwen@uu.nl}

\begin{abstract}
...
\end{abstract}

\keywords{item response theory, goodness-of-fit tests, fit indices, power, empirical alpha}

\maketitle

\section{Introduction}
Item Response Theory (IRT) is an often used tool for designing and analysing tests or questionnaires in psychological, educational and organisational practices. The models in IRT measure latent traits -- characteristics that are not directly observable (e.g., attitudes, intelligence, etc.) -- through analysing answers to a test or questionnaire. The goal in IRT is to find the most parsimonious model that best describes the scores on the test items. To achieve this, the model must show a good fit. If a model is used that does not fit the data, it can lead to dire consequences such as faults in the validity of your measurement (Cri\c{s}an et al., 2017; Jiao \& Lau, 2003; Zhao \& Hambleton, 2017) and by extension your conclusion. Therefore, model fit should be assessed after fitting an IRT model to test/questionnaire data. \\
\indent There are mainly two procedures that can be applied to quantify model fit. First, model fit can be assessed through goodness-of-fit tests. Goodness-of-fit tests are statistical hypothesis tests that describe how well the observed data follows the expected data under a given model. \\
\indent Commonly used goodness-of-fit tests in IRT are a $\chi^2$-difference test and Pearson's $\chi^2$-test. The $\chi^2$-difference and Pearson's $\chi^2$-test, however, both suffer from issues. Pearson's test, for example, will not follow a $\chi^2$-distribution when many score patterns are missing or have a low frequency, which is often the case -- especially as test length increases. The $\chi^2$-difference test instead is difficult to use for two often used IRT models: the two- (2PL) and three-parameter logistic (3PL) model. These models both specify the probability of scoring a dichotomous multiple choice item correctly as a logistic distribution dependent on item characteristics and a latent variable. The reason the $\chi^2$-difference test is difficult to use for these models, is because the 3PL and its generalisations tend to suffer from consistency and estimation issues (Barton \& Lord, 1981; Loken \& Rulison, 2010). Another concern with the tests is power, which can be viewed as both a blessing and a curse. This is because many goodness-of-fit tests suffer from a lack of power when sample size is low. However, when sample size increases, the power to reject any reasonable model that does not perfectly fit, also increases (Curran et al., 1996). These findings suggest that there is a lack of usable goodness-of-fit tests to test certain IRT models. \\
\indent Consequently, fit indices were introduced as a second procedure to assess model fit. Fit indices are mathematical descriptives that indicate how well a model fits to the data. Note, however, that fit indices alone are not sufficient to infer whether a model fits well as they are not an inferential statistic and only describe the overall fit. Taken together with goodness-of-fit tests, fit indices do allow researchers to have a more comprehensive overview of model fit. For example, when due to a lot of power, a goodness-of-fit test shows that the model should be rejected, fit indices can indicate whether the model is still reasonable. \\ 
\indent Previous research in IRT has seen the development of multiple fit indices. (e.g., $Y\text{-}Q_1$; Yen, 1981, $\text{RMSEA}_n$; Maydeu-Olivares \& Joe, 2014). For an overview of the use and limitations of several IRT fit indices, see Nye et al. (2020). Fit indices from Structural Equation Modeling can also be used in IRT. Although research has been done to examine the performance of a few fit indices from Structural Equation Modeling in IRT (e.g., RMSEA \& SRMR; Maydeu-Olivares \& Joe, 2014), hardly any research has been done towards the use of the Tucker-Lewis Index (TLI; Tucker \& Lewis, 1973) and the Comparative Fit Index (CFI; Bentler, 1990) in an IRT context. Only one paper examines the CFI (Yang, 2020) and two papers investigate the TLI (Cai et al., 2021; Yang, 2020) in an IRT setting. Both studies concluded that the two fit indices can add addititional insights in assessing model fit. However, there have been no studies that investigate the performance of the TLI and CFI when calculated through the $\chi^2$ statistic and with a baseline model that allows for independence between test items. The calculations of the fit indices influence their performance and this should therefore be investigated further. \\
\indent To summarise, in the field of IRT there are not enough usable goodness-of-fit tests available to test the 2- and 3PL model and scarce studies investigating the performance of the TLI and CFI. We address these two issues by developing and evaluating a new Likelihood Ratio (LR) goodness-of-fit test, named the LR Randomisation test, and assessing the performance of the TLI and CFI based on calculations not researched before in an IRT setting.  

\subsection{The present study}
In order to evaluate the LR Randomisation test for the 2- and 3PL in IRT and to better understand the possible uses of the CFI and TLI in an IRT setting, the present study answers the following three research questions through a simulation study:
\begin{enumerate}
\item{What sample size is necessary at different test lengths for the LR Randomisation test to perform well?}
\item{How does the performance of the LR Randomisation test compare to the performance of a $\chi^2$-difference and Pearson's $\chi^2$-test?}
\item{What is the performance of the TLI and CFI in IRT?}
\end{enumerate}
Results from the simulation study are reported to answer the research questions. Furthermore, empirical data from the Law School Admission Test (LSAT) are used to illustrate the value of the LR Randomisation test, the TLI and the CFI.

\section{Methods}
Before we share the methodology of the simulation study, let us first examine a brief summary on the statistical theory associated with our study. In IRT, the goal is to find a model that best describes scores on test items. To achieve this, IRT presupposes three assumptions: (1) conditional independence of items given the latent trait, denoted by $\theta$, (2) independence of observations and (3) the response to an item can be modeled by an item response function (IRF). An IRF is a mathematical equation that relates the probability to score a certain category on an item to $\theta$. Below, you find the IRF for the 3PL (Birnbaum, 1968):

\begin{equation}
P(X_i = 1 | \theta, \alpha_{i}, \beta_{i}, \gamma_{i}) = \gamma_{i} + (1 - \gamma_{i}) \cdot 
\frac{e^{\alpha_{i}\theta - \beta_{i}}}{1 + e^{\alpha_{i}\theta - \beta_{i}}},
\end{equation}

where $X_i$ is a random variable indicating the response to item $i$. The probability of scoring a 1 on item $i$ in the 3PL depends on (a) the latent variable $\theta$, (b) the location parameter of the item $\beta_{i}$, which denotes how difficult the item is, (c) the scaling parameter of the item $\alpha_{i}$, which shows how well item $i$ discriminates between individuals who score a 0 and individuals who score a 1, and (d) an item-specific lower asymptote $\gamma_{i}$, which indicates whether there is a baseline probability of scoring a 1 (e.g., a multiple choice test with 4 options has a .25 baseline probability of scoring a 1). The 2PL is a special case of the 3PL, where $\gamma_{i}$ equals 0 for all items. \\
\indent Combining an IRF with the assumption of conditional independence allows us to model the probability of a complete score pattern to $k$ items simply by factoring the probabilities for each item:

\begin{equation}
P(\boldsymbol{X} = \boldsymbol{x} | \theta, \boldsymbol{\nu}) = \prod_{i=1}^{k} \{P(X_i = 1 | \theta, \boldsymbol{\nu})\}^{x_i} \cdot  \{1 - P(X_i = 1 | \theta, \boldsymbol{\nu}) \}^{1 - x_i},
\end{equation}

where $\boldsymbol{X}$ is now a random vector indicating a score pattern and $\boldsymbol{x}$ is the realisation of $\boldsymbol{X}$. Note that $\boldsymbol{\nu}$ is a vector containing item parameters for all $k$ items. We can take this even further by taking into account the assumption that persons are randomly sampled from a population. The joint marginal probability of a score pattern of a randomly sampled individual then becomes:

\begin{equation}
P(\boldsymbol{X} = \boldsymbol{x}) = \int \prod_{i=1}^{k} \{ P(\boldsymbol{X} = \boldsymbol{x} | \theta, \boldsymbol{\nu}) \} \,\phi(\theta)\,d\theta,
\end{equation}

where $\phi(\theta)$ is the univariate density of the latent variable $\theta$. In order to solve this equation, the density of $\theta$ has to be specified. For example, $\phi(\theta)$ can be specified as a standard normal distribution. With the joint marginal probability and the assumption of independence of observations, we can construct a likelihood function and estimate $\boldsymbol{\nu}$ through marginal maximum likelihood estimation:

\begin{equation}
\mathcal{L}(\boldsymbol{\nu}) = \prod_{\boldsymbol{x}} (P(\boldsymbol{X} = \boldsymbol{x}))^{n_{\boldsymbol{x}}},
\end{equation}

where $n_{\boldsymbol{x}}$ is the frequency of score pattern $\boldsymbol{x}$. Maximisation of $\mathcal{L}(\boldsymbol{\nu})$ would then lead to the estimation of the item parameters $\boldsymbol{\hat{\nu}}$. This is generally how in IRT a model is fitted to data. \\
\indent After a model has been fitted as described above, the next step is to test how well the model fits the data. There are multiple options to assess model fit. Usually, both goodness-of-fit tests and fit indices are used. Commonly used goodness-of-fit tests are the $\chi^2$-difference test and the Pearson's $\chi^2$-test. The $\chi^2$-difference test uses the following likelihood statistic:

\begin{equation}
\frac{\mathcal{L}_0}{\mathcal{L}_a},
\end{equation}

where $\mathcal{L}_0$ is the likelihood of the model you fit (i.e., the null model) and $\mathcal{L}_a$ is the likelihood of an alternative model under which the null model has to be nested. According to Willks' theorem (Wilks, 1938), under the null hypothesis this likelihood statistic will asymptotically follow a $\chi^2$ distribution as sample size increases. Alternatively, Pearson's $\chi^2$-test is based on the statistic below: 

\begin{equation}
\sum_{\boldsymbol{x}} \frac{(O_{\boldsymbol{x}} - E_{\boldsymbol{x}})^2}{E_{\boldsymbol{x}}},
\end{equation}

where $O_{\boldsymbol{x}}$ is the observed frequency for score pattern $\boldsymbol{x}$ and $E_{\boldsymbol{x}}$ is the expected frequency for score pattern $\boldsymbol{x}$ given a model. This statistic also asymptotically follows a $\chi^2$ distribution. The goodness-of-fit test that we develop and test in the present study is based on the hereunder likelihood statistic: 

\begin{equation}
\frac{max(\mathcal{L}_0)}{\prod_{j = 1}^g max(\mathcal{L}_j)},
\end{equation}
 
where $L_0$ is the likelihood of the chosen model for the whole dataset and $L_j$ is the likelihood of the chosen model for group $j$, which is gained by randomly assigning the observations to $g$ groups. Wilks' theorem also applies to this likelihood statistic. Because all three aforementioned tests asymptotically follow a $\chi^2$ distribution, they can be used for Null Hypothesis Significance Testing. \\
\indent For fit indices, we research the performance of the TLI and CFI. These indices are estimated through the use of a baseline and saturated model. Our baseline model is a complete-independence model, which has the following IRF:

\begin{equation}
P(X_i = 1 | \beta_{i}) = \frac{e^{- \beta_{i}}}{1 + e^{- \beta_{i}}},
\end{equation}

where the probability of scoring a 1 on item $i$ is dependent only on the difficulty of the item and no longer on a latent variable. We argue that this is an appropriate baseline model, because the IRF entails that the joint probability distribution is simply the product of the marginal probability distributions and the items are independent. In this baseline model, the probability of scoring a 1 on item $i$ is simply the proportion of people who score a 1 on item $i$. From this, the maximum likelihood estimate of $\beta_{i}$, denoted by $\hat{\beta_{i}}$, can then mathematically be derived to:


\begin{equation*}
\hat{\beta_{i}} = ln(\frac{n - n_i}{n_i}), 
\end{equation*}

where $n_i$ is the number of observations who scored a 1 on item $i$ and $n$ is the total number of observations. In the saturated model there are as many parameters as data points, which leads to a perfect fit. We choose the following saturated model:

\begin{equation}
P(\boldsymbol{X} = \boldsymbol{x}) = \pi_{\boldsymbol{x}},
\end{equation}

where there no longer is an IRF. Instead, perfect model fit is gained by allowing each score pattern to have their own parameter ($\pi_{\boldsymbol{x}}$). The maximum likelihood estimate of $\pi_{\boldsymbol{x}}$, denoted by $\hat{\pi_{\boldsymbol{x}}}$, is then the relative frequency of the score pattern:

\begin{equation*}
\hat{\pi_{\boldsymbol{x}}} = \frac{n_{\boldsymbol{x}}}{n},
\end{equation*}

where $n_{\boldsymbol{x}}$ is the number of observations with score pattern $\boldsymbol{x}$ and $n$ is the total number of observations. Using both the baseline and saturated model, the fit indices can be calculated through the following formulae:

\begin{equation}
\text{CFI} = 1 - \frac{\text{max}\{(\chi^2_T - df_T), 0\}}{\text{max}\{(\chi^2_T - df_T), (\chi^2_0 - df_0), 0\}},
\end{equation}
\begin{equation}
\text{TLI} = \frac{\chi^2_0/df_0 - \chi^2_T/df_T}{\chi^2_0/df_0 - 1}.
\end{equation}

In both equations, $\chi^{2}_{T}$ is the result of a $\chi^2$-difference test between the tested model and the saturated model with $df_T$ degrees of freedom, and $\chi^{2}_{0}$ is the outcome of a $\chi^2$-difference test between the baseline model and the saturated model with $df_0$ degrees of freedom.

Finally, 

-- NEW FIT INDEX -- (with proof that it stays between 0 - 1)

\section{Simulation study I}

In the present study, we consider IRT with unidimensional $\theta$, dichotomous test items and the IRF for the 2- and 3PL. In order to evaluate the performance of the LR Randomisation test and the TLI and CFI in this setting, we conduct a simulation study. Specifically, for the LR Randomisation test we investigate whether whether the test is robust and at which rate the test is able to reject a misspecified model under different conditions. Furthermore, we research how well the LR Randomisation test performs compared to a $\chi^2$-difference test and Pearson's $\chi^2$-test. For the TLI and CFI, we investigate their values under correct and incorrect model specification. The aim of this is to examine whether the fit indices are a useful addition for assessing model fit in IRT modelling.

\subsection{Data generation}
Data generation starts by first sampling the person parameters $\theta$ from a standard normal distribution. Then, either the 2PL or 3PL is chosen as basis for the data generation (see below). We choose to keep item parameters static over all simulations. For the difficulty parameter $\beta_i$, we choose the values [-2, -1, 0, 1, 2]. As for the discrimination parameter $\alpha_i$, we choose repetitions of the values [0.7, 0.85, 1, 1.15, 1.3]. To create a more realistic scenario, these two parameters were then matched with one another in order to make sure that for every item difficulty, there are low and high discriminating items. Finally, when the data generating model is the 3PL, we choose the values [.09, 0.12, 0.15, 0.18, 0.21] per five items for the pseudo-guessing parameter $\gamma_i$. Then, probabilities are estimated for all items on a test, given $\theta$, the chosen model and item parameters. Finally, a matrix of simulated responses to a dichotomous test is created by sampling from a binomial distribution using the estimated probabilities. 
\subsection{Simulation design}
In the simulation study, we vary four factors: test length, sample size, model types and number of groups. For an overview of the conditions we used for the factors, see \textit{Table \ref{tab:1}}. The simulation design results in a total of 3 (test length) x 5 (sample size) x 2 (model type) = 30 conditions. In each replication of each condition, we fit the 2PL model. Models are fitted using functions from the \textit{ltm} package in R (Rizopoulos, 2006), which approximates marginal maximum likelihood estimation through the Gauss-Hermite quadrature rule. Then we obtain the results of the TLI, CFI and the three different types of goodness-of fit tests: (1) a $\chi^2$-difference test, which is obtained by testing the 2PL under the 3PL with no constraints, (2) Pearson's $\chi^2$-test, which is estimated through aggregating score patterns from the data and comparing the observed score pattern frequencies to the expected score pattern frequencies under the given model, and (3) the LR Randomisation test with a varying number of groups. We replicate each simulation condition 300 times.

\begin{table}[t!]
\caption{Overview of Simulation Conditions for Each Factor}
\begin{tabular}{ c c c }
\toprule
Factor & Conditions & Description \\
 \\
\midrule
\multicolumn{1}{l}{Test length} & 5 - 10 - 20 & \multicolumn{1}{l}{\shortstack{The total number of items that \\ the test will consist of}} \\ \\ 
\multicolumn{1}{l}{Sample size} & \shortstack{100 - 200 - 500 \\ 1000 - 1500} & \multicolumn{1}{l}{\shortstack{The total number of observations \\ that are available for each item}} \\ \\
\multicolumn{1}{l}{Model type} & 2PL - 3PL & \multicolumn{1}{l}{\shortstack{The models that we will use as \\ the basis for data generation}} \\ \\
\multicolumn{1}{l}{Number of groups} & 2 - 3 - 4 & \multicolumn{1}{l}{\shortstack{The number of groups that the data \\ gets divided into for the LR \\ Randomisation test calculations}} \\

\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. 2PL = two-parameter logistic model; 3PL = \\ three-parameter logistic model.
\label{tab:1}
\end{table}

\subsection{Performance metrics}
We study performance of the different goodness-of-fit tests by estimating both type I error and power. Both values are estimated by obtaining the detection rate, which is the number of times the null hypothesis is rejected divided by the total number of replications per condition. Power is estimated when generating data under the 3PL and fitting the 2PL model. Type I error is estimated when generating data under the 2PL and fitting the 2PL model. With these values, we compare the different type of tests with one another under every condition, where a test with lower power or higher type I error is noted as performing worse. For all tests, we chose a level of significance of $\alpha = .05$. \\
\indent To measure the performance of the TLI and CFI, we estimate the mean and standard error (SE) of each fit index under every condition. Then, we can inspect whether the average TLI and CFI values decrease in the conditions where data is generated under the 3PL, compared to when data is generated under the 2PL. 

\subsection{Results}

To answer the three research questions related to the performance of the LR Randomisation test and the TLI and CFI in the IRT paradigm, we conducted the above described simulation study. The ...

\subsubsection{Goodness-of-fit tests}

\textit{Table \ref{tab:2}} shows the empirical rejection rates at the .05 $\alpha$ level for the different $\chi^2$-based goodness-of-fit tests. For the LR Randomisation test, we can observe the asymptomatic clearly. As sample size increases, the empirical $\alpha$ decreases from .10 to around the nominal level .05. This also means that for low sample sizes, the test statistic does not yet completely follow a $\chi^2$ distribution. For Pearson's $\chi^2$ test, we can also clearly see that as the number of items increases, the test strays further from a $\chi^2$ distribution, as demonstrated by the increasing empirical $\alpha$ values. This is in accordance with the issues described in the introduction.

The rejection rates of the $\chi^2$ statistic seems to not deviate severely from the nominal level. Furthermore, there seem to be no large differences between the different sample size and test length conditions. Therefore, we can  state that the $\chi^2$ statistics approximately follow their supposed $\chi^2$ distribution. We can also take these results as a form of confirmation for the theory and design behind our study. Important to remember, however, is that empirical data might not be as polished.

\textit{Table \ref{tab:3}} shows the rejection rates for when the data generating model is not the true model. In other words, this table shows the estimated statistical power values for the different goodness-of-fit tests. 

\subsubsection{Fit indices}


% Table for Empirical Alpha results
\begin{table}[ht]
\caption{Empirical Alpha estimates for different goodness-of-fit tests}
\begin{tabular}{ r r | a c a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{5}{c}{Goodness-of-fit test} \\
\rowcolor{white} \textit{I} & \textit{N} & LR2 & LR3 & LR4 & $\chi^2$ & P-$\chi^2$ \\
\midrule
5 & 200 & 0.09 & 0.06 & 0.10 & 0.05 & 0.06 \\ 
& 300 & 0.07 & 0.08 & 0.07 & 0.04 & 0.04 \\
& 500 & 0.07 & 0.07 & 0.06 & 0.03 & 0.05 \\
& 1000 & 0.04 & 0.06 & 0.07 & 0.03 & 0.06 \\
& 1500 & 0.07 & 0.07 & 0.06 & 0.02 & 0.04 \\
10 & 200 & 0.08 & 0.11 & 0.11 & 0.03 &  0.22 \\ 
& 300 & 0.04 & 0.05 & 0.09 & 0.04 & 0.020 \\
& 500 & 0.04 & 0.05 & 0.06 & 0.06 & 0.17 \\
& 1000 & 0.02 & 0.06 & 0.06 & 0.03 & 0.12 \\
& 1500 & 0.03 & 0.05 & 0.04 & 0.02 & 0.11 \\
20 & 200 & 0.07 & 0.08 & 0.12 & 0.06 & -- \\ 
& 300 & 0.05 & 0.08 & 0.10 & 0.04 & -- \\
& 500 & 0.07 & 0.08 & 0.06 & 0.03 & -- \\
& 1000 & 0.05 & 0.05 & 0.05 & 0.02 & -- \\
& 1500 & 0.04 & 0.05 & 0.05 & 0.03 & -- \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = two-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; LR2 = LR Randomisation test with $g = 2$; LR3 = LR Randomisation test with $g = 3$; LR4 = LR Randomisation test with $g = 4$; $\chi^2$ = $\chi^2$-difference test under the three-parameter logistic model with no constraints; P-$\chi^2$ = Pearson's $\chi^2$ test.
\label{tab:2}
\end{table}

\newpage

\begin{table}[ht]
\caption{Power estimates for different goodness-of-fit tests}
\begin{tabular}{ r r | a c a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{5}{c}{Goodness-of-fit test} \\
\rowcolor{white} \textit{I} & \textit{N} & LR2 & LR3 & LR4 & $\chi^2$ & P-$\chi^2$ \\
\midrule
5 & 200 & 0.10 & 0.09 & 0.12 & 0.06 & 0.06 \\ 
& 300 & 0.09 & 0.08 & 0.14 & 0.08 & 0.06 \\
& 500 & 0.07 & 0.09 & 0.12 & 0.11 & 0.04 \\
& 1000 & 0.06 & 0.05 & 0.08 & 0.12 & 0.07 \\
& 1500 & 0.05 & 0.07 & 0.05 & 0.19 & 0.07 \\
10 & 200 & 0.09 & 0.14 & 0.20 & 0.25 & 0.29 \\ 
& 300 & 0.11 & 0.11 & 0.13 & 0.26 & 0.21 \\
& 500 & 0.05 & 0.09 & 0.08 & 0.32 & 0.22 \\
& 1000 & 0.08 & 0.07 & 0.07 & 0.58 & 0.16 \\
& 1500 & 0.06 & 0.06 & 0.06 & 0.75 & 0.22 \\
20 & 200 & 0.10 & 0.11 & 0.16 & 0.52 & -- \\ 
& 300 & 0.08 & 0.10 & 0.11 & 0.66 & -- \\
& 500 & 0.05 & 0.08 & 0.08 & 0.89 & -- \\
& 1000 & 0.03 & 0.05 & 0.06 & 0.97 & -- \\
& 1500 & 0.03 & 0.05 & 0.07 & 1.00 & -- \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = three-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; LR2 = LR Randomisation test with $g = 2$; LR3 = LR Randomisation test with $g = 3$; LR4 = LR Randomisation test with $g = 4$; $\chi^2$ = $\chi^2$-difference test under the three-parameter logistic model with no constraints; P-$\chi^2$ = Pearson's $\chi^2$ test.
\label{tab:3}
\end{table}

\newpage


\begin{table}[ht]
\caption{TLI and CFI values under correct model specification}
\begin{tabular}{ r r | a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{1}{c}{TLI} & \multicolumn{1}{c}{CFI} & \multicolumn{1}{c}{NFI} \\
\textit{I} & \textit{N} & M (SE) & M (SE) & M (SE) \\
\midrule
 5 & 200 & 0.63 (0.19) & 0.75 (0.12) & 1.00 (0.00) \\
& 300 & 0.74 (0.14) & 0.82 (0.09) & 1.00 (0.00) \\
& 500 & 0.84 (0.09) & 0.90 (0.06) & 1.00 (0.00) \\
& 1000 & 0.92 (0.05) & 0.94 (0.03) & 1.00 (0.00) \\
& 1500 & 0.95 (0.03) & 0.96 (0.02) & 1.00 (0.00) \\
10 & 200 & 0.06 (0.06) & 0.23 (0.05) & 0.72 (0.03) \\
& 300 & 0.11 (0.05) & 0.27 (0.04) & 0.82 (0.02) \\
& 500 & 0.19 (0.04) & 0.33 (0.04) & 0.90 (0.01) \\
& 1000 & 0.32 (0.03) & 0.45 (0.03) & 0.97 (0.00) \\
& 1500 & 0.42 (0.03) & 0.53 (0.03) & 0.99 (0.00)\\
20 & 200 & 0.04 (0.02) & 0.14 (0.02) & 0.03 (0.01) \\
& 300 & 0.05 (0.02) & 0.14 (0.02) & 0.03 (0.01) \\
& 500 & 0.05 (0.01) & 0.14 (0.01) & 0.05 (0.01) \\
& 1000 & 0.07 (0.01) & 0.16 (0.01) & 0.08 (0.01) \\
& 1500 & 0.08 (0.01) & 0.17 (0.01) & 0.10 (0.01) \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = two-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; TLI = tucker-lewis index; CFI = comparative fit index; M = mean; SE = standard error.
\label{tab:4}
\end{table}

\newpage

\begin{table}[ht]
\caption{TLI and CFI values under incorrect model specification}
\begin{tabular}{ r r | a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{1}{c}{TLI} & \multicolumn{1}{c}{CFI} & \multicolumn{1}{c}{NFI} \\
\textit{I} & \textit{N} & M (SE) & M (SE) & M (SE) \\
\midrule
 5 & 200 & 0.33 (0.31) & 0.55 (0.19) & 1.00 (0.00) \\
& 300 & 0.49 (0.24) & 0.66 (0.16) & 1.00 (0.00) \\
& 500 & 0.66 (0.18) & 0.77 (0.12) & 1.00 (0.00) \\
& 1000 & 0.81 (0.11) & 0.82 (0.07) & 1.00 (0.00) \\
& 1500 & 0.86 (0.08) & 0.91 (0.05) & 1.00 (0.00) \\
10 & 200 & 0.00 (0.04) & 0.11 (0.03) & 0.73 (0.04) \\
& 300 & 0.00 (0.04) & 0.13 (0.03) & 0.81 (0.02) \\
& 500 & 0.00 (0.03) & 0.17 (0.03) & 0.89 (0.01) \\
& 1000 & 0.07 (0.03) & 0.24 (0.02) & 0.96 (0.00) \\
& 1500 & 0.15 (0.03) & 0.31 (0.02) & 0.98 (0.00) \\
20 & 200 & 0.00 (0.01) & 0.06 (0.01) & 0.03 (0.01) \\
& 300 & 0.00 (0.01) & 0.06 (0.01) & 0.04 (0.01) \\
& 500 & 0.00 (0.01) & 0.07 (0.01) & 0.06 (0.01) \\
& 1000 & 0.00 (0.01) & 0.07 (0.01) & 0.09 (0.01) \\
& 1500 & 0.00 (0.01) & 0.08 (0.01) & 0.12 (0.01) \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = three-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; TLI = tucker-lewis index; CFI = comparative fit index; M = mean; SE = standard error.
\label{tab:5}
\end{table}

NOTES FOR THESE RESUTLS: for low item lengths and low sample sizes, high SE's, therefore a simple point value might not do to assess model fit. Bootstrapping might be required. High sample sizes this is not an issue however. 
NOTE that negative values were sometimes seen in the miss fit condition, this is normal (citation used in TLI IRT)

\newpage

\section{Simulation Study II}

\subsection{Results}

\section{Empirical example}
To examine the possible uses of our LR Randomisation test, the TLI and CFI, we estimated and interpreted their values on a real-life dataset, gained from XX, which contains information about XXX ($N = ...$). The dataset has XX items on a dichotomous scale that together measure XXX. We fitted the 3PL model with XX.

\section{Discussion}

\subsection{Limitations}

- didn't test 3PL
- didn't test multidimensional IRT
- didn't test polytome IRT

future research can do these, also the test can be used in any field of statistics. not just IRT. First findings show promise of the test and might be worth while. 


\subsection{Conclusion}

\newpage

\nocite{*}
\bibliographystyle{apalike}
\bibliography{reportref}

\end{document}
