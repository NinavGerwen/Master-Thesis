% setting document class according to SAGE guidelines
\documentclass[Royal,sageapa,times,doublespace]{sagej}

% packages for APA style requirements according to SAGE guidelines
\usepackage{moreverb,url}
\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}

% mathematical (equation) packages
\usepackage{amsmath}
\usepackage{amssymb}

% table-related packages for APA style tables
\usepackage{multirow,booktabs,setspace,caption}
\usepackage{tikz}
\usepackage{xcolor, colortbl}

% some document-wide table settings to be able to use
\definecolor{Gray}{gray}{0.90}
\newcolumntype{a}{>{\columncolor{Gray}}c}

% document settings according to SAGE guidelines
\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\volumeyear{2023}

% beginning the manuscript
\begin{document}

\runninghead{van Gerwen}% and Hessen}

\title{Assessing Fit of Item Response Theory Models using a Multi-Group Likelihood Ratio Test and Fit Indices}

\author{Nina van Gerwen \affilnum{1}} % and Dave Hessen\affilnum{2}}

\affiliation{\affilnum{1}Utrecht University, NL} % \\
% \affilnum{2}Utrecht University, NL}

\corrauth{Nina van Gerwen, Utrecht University,
Faculty of Social Sciences, Department of Methodology and Statistics,
Padualaan 14, Utrecht, 3584 CH, NL.}

\email{n.l.vangerwen@uu.nl}

\begin{abstract}
In Item Response Theory (IRT), it is important to investigate the appropriateness of the models you fit to the data in order to have valid measurements and conclusions. The appropriateness of a model is usually assessed through goodness-of-fit tests and fit indices. However, for dichotomous IRT models, common goodness-of-fit tests suffer from various issues (Barton \& Lord, 1981; Loken \& Rulison, 2010). Previous research has also shown that many fit indices in IRT are also unable to determine model misspecification when fitting the two-parameter logistic model (2PL) (Nye et al., 2020). We further the research by investigating a goodness-of-fit test based on multiple group analysis, the MG LR test, and three scarcely studied fit indices: (1) the Tucker Lewis index (TLI; Tucker \& Lewis, 1973), (2) Comparative Fit index (CFI; Bentler, 1990) and (3) the Identity Coefficient Fit index (ICFI; Zegers \& ten Berge, 1985). We examine the performance of these fit measures through a simulation study. The results suggests that the ICFI, Pearson's $\chi^2$ and the MG LR test are unable to determine model misspecification when fitting the 2PL. In contrast, the TLI and CFI showed good performance in determining model misspecification in dichotomous IRT. A follow-up simulation study showed that the MG LR test is effective in detecting group differences, while insensitive to violations of correct model specification. We believe this to be a unique feature of the test and speculate its value as an instrument in detecting group differences. We also recommend that the TLI and CFI become more standard in use in order to improve model appraisal in IRT.
\end{abstract}

\keywords{item response theory, goodness-of-fit tests, fit indices, power, group differences}

\maketitle

\section{Introduction}
Item Response Theory (IRT) is an often used instrument for designing and analysing tests/questionnaires in psychological, educational and organisational practices. The models in IRT infer latent traits -- characteristics that are not directly observable (e.g., attitudes, intelligence, etc.) -- through analysing the answers to a test. The goal in IRT is to find the most parsimonious model that best describes the scores on the test items. To achieve this, the model must show a good fit. If a model is used that does not fit the data, it can lead to dire consequences such as faults in the validity of your measurement (Cri\c{s}an et al., 2017; Jiao \& Lau, 2003; Zhao \& Hambleton, 2017) and by extension your conclusion. Therefore, model fit should be assessed after fitting an IRT model to test data. \\
\indent There are mainly two procedures that can be applied to quantify model fit. First, model fit can be assessed through goodness-of-fit tests. Goodness-of-fit tests are statistical hypothesis tests that describe how well the observed data follow the expected data given a model. \\
\indent Common goodness-of-fit tests in IRT are a $\chi^2$-difference test and Pearson's $\chi^2$ test. However, these tests suffer from issues. Pearson's $\chi^2$ test, for example, will not follow a $\chi^2$ distribution when many score patterns are missing or have a low frequency, which is often the case -- especially as test length increases. The $\chi^2$-difference test instead is difficult to use for two dichotomous IRT models: the two- (2PL) and three-parameter logistic (3PL) model. These models both specify the probability of scoring a dichotomous item correctly as a logistic function of item characteristics and a latent trait. However, the 3PL and its generalisations tend to suffer from consistency and estimation issues (Barton \& Lord, 1981; Loken \& Rulison, 2010), which is why the $\chi^2$-difference test is hard to use with the models. Another concern with the tests is power, which can be viewed as both a blessing and a curse. This is because many goodness-of-fit tests suffer from a lack of power when sample size is low. However, when sample size increases, the power to reject any reasonable model that does not perfectly fit, also increases (Curran et al., 1996). These findings suggest that there is a lack of usable goodness-of-fit tests to test certain IRT models. \\
\indent Therefore, we explore a new possible goodness-of-fit test to assess model misspecification in dichotomous IRT. The test we are interested in originates from multi-group analyses, where it can be used to detect group differences in existing groups. However, what if there were no existing groups, and groups are made by random assignment instead. In this scenario, if a wrong model is specified, would the test have power to reject this model in favour of the true model? We expect that the test will have power to detect this model misspecification. If the test does not turn out to have any power, it would mean that the test is insensitive to incorrect model specification. If the interest is then in testing for differences between existing groups, it means that the test ignores model misspecification and tests solely for group differences. \\
\indent The second procedure of assessing model fit is through the use of fit indices. Fit indices are mathematical descriptives that indicate how close the fit of a model is to a perfect fit. Note, however, that fit indices alone are not sufficient to infer whether a model fits well as they are not inferential statistics and only describe the overall fit. Taken together with goodness-of-fit tests, fit indices do allow researchers to have a more comprehensive overview of model fit. For example, when due to a lot of power, a goodness-of-fit test shows that the model should be rejected, fit indices can indicate whether the model is still reasonable. \\ 
\indent Previous research in IRT has seen the development of multiple fit indices. (e.g., $Y\text{-}Q_1$; Yen, 1981, $\text{RMSEA}_n$; Maydeu-Olivares \& Joe, 2014). For an overview of the use and limitations of several IRT fit indices, see Nye et al. (2020). One limitation of many fit indices was that they exhibited poor performance when the 2PL was fitted to the data. Fit indices from Structural Equation Modeling (SEM) can also be used in IRT. Although the performance of many fit indices from SEM have been researched in IRT (e.g., RMSEA \& SRMR; Maydeu-Olivares \& Joe, 2014), hardly any past research investigated the potential of the Tucker-Lewis Index (TLI; Tucker \& Lewis, 1973) and Comparative Fit Index (CFI; Bentler, 1990) in IRT. For this setting, only one paper has examined the CFI (Yang, 2020) and two papers have investigated the TLI (Cai et al., 2021; Yang, 2020). Both studies concluded that the fit indices were able to add additional insights in assessing model fit. However, there have been no studies that investigate the performance of the TLI and CFI in evaluating model with a baseline model that assumes independence between test items and while calculated through the $\chi^2$ statistic appropriateness. These factors influence the performance of the fit indices, which is why we research the TLI and CFI further in dichotomous IRT. \\
\indent Additionally, we research another possible fit index not investigated before in the field of IRT. This fit index is based on the identity coefficient by Zegers \& ten Berge (1985), which indicates the degree to which two vectors are identical. Therefore, this coefficient can reflect the degree to which the observed score pattern frequencies are aligned with the expected score pattern frequencies given a certain model. \\
\indent To summarise, in dichotomous IRT there are not enough measures of model fit available and scarce studies investigating the performance of the TLI and CFI. We address these two issues by developing and evaluating a new Likelihood Ratio (LR) goodness-of-fit test based on multi-group analysis, named the Multiple Group LR (MG LR) test, and assessing the performance of the TLI and CFI based on new calculations. We also examine the performance of a new fit index inspired by the identity coefficient by Zegers \& ten Berge (1985), named the identity coefficient fit index (ICFI). Specifically, we answer and discuss the following four research questions through multiple simulation studies and an empirical example:
\begin{enumerate}
\item{What sample size is necessary at different test lengths for the MG LR test to approximately follow a $\chi^2$ distribution?}
\item{What is the power of the MG LR test to detect model misspecification when groups are based on random assignment when fitting the 2PL?}
\item{Under random group assignment, how does the performance of the MG LR test compare to the performance of a $\chi^2$-difference and Pearson's $\chi^2$ test in detecting model misspecification when working with the 2PL?}
\item{What is the performance of the TLI, CFI and ICFI in assessing model fit when the 2PL is fitted?}
\end{enumerate}

\section{Methods}
In IRT, the goal is to find a model that best describes scores on test items. To achieve this, IRT presupposes three assumptions: (1) conditional independence of items given the latent trait, denoted by $\theta$, (2) independence of observations and (3) the response to an item can be modeled by an item response function (IRF). An IRF is a mathematical equation that relates the probability to score a certain category on an item to $\theta$. The IRF for the 3PL (Birnbaum, 1968) is:

\begin{equation}
P(X_i = 1 | \theta, \alpha_{i}, \beta_{i}, \gamma_{i}) = \gamma_{i} + (1 - \gamma_{i}) \cdot 
\frac{e^{\alpha_{i}\theta - \beta_{i}}}{1 + e^{\alpha_{i}\theta - \beta_{i}}},
\end{equation}

where $X_i$ is a random variable indicating the response to item $i$. The probability of scoring a 1 on item $i$ in the 3PL depends on (a) the latent trait $\theta$, (b) the location parameter of the item $\beta_{i}$, which denotes how difficult the item is, (c) the scaling parameter of the item $\alpha_{i}$, which shows how well item $i$ discriminates between individuals with lower and higher $\theta$, and (d) an item-specific lower asymptote $\gamma_{i}$, which indicates whether there is a baseline probability of scoring a 1 (e.g., a multiple choice question with 4 options has a .25 baseline probability of scoring a 1). The 2PL is a special case of the 3PL, where $\gamma_{i}$ equals 0 for all items. \\
\indent Combining an IRF with the assumption of conditional independence allows us to model the probability of a score pattern to $k$ items simply by factoring the probabilities for each item:

\begin{equation}
P(\boldsymbol{X} = \boldsymbol{x} | \theta, \boldsymbol{\nu}) = \prod_{i=1}^{k} \{P(X_i = 1 | \theta, \boldsymbol{\nu})\}^{x_i} \cdot  \{1 - P(X_i = 1 | \theta, \boldsymbol{\nu}) \}^{1 - x_i},
\end{equation}

where $\boldsymbol{X}$ is a random vector indicating a score pattern and $\boldsymbol{x}$ is the realisation of $\boldsymbol{X}$. Note that $\boldsymbol{\nu}$ is a vector containing item parameters for all $k$ items. By assuming that persons are randomly sampled from a population, the joint marginal probability of a score pattern of an individual then becomes:

\begin{equation}
P(\boldsymbol{X} = \boldsymbol{x}) = \int P(\boldsymbol{X} = \boldsymbol{x} | \theta, \boldsymbol{\nu}) \,\phi(\theta)\,d\theta,
\end{equation}

where $\phi(\theta)$ is the univariate density of the latent trait $\theta$. In order to solve this equation, the density of $\theta$ has to be specified. For example, $\phi(\theta)$ can be specified as a standard normal distribution. With the joint marginal probability and the assumption of independence of observations, we can construct a likelihood function and estimate $\boldsymbol{\nu}$ through marginal maximum likelihood estimation:

\begin{equation}
\mathcal{L}(\boldsymbol{\nu}) = \prod_{\boldsymbol{x}} (P(\boldsymbol{X} = \boldsymbol{x}))^{n_{\boldsymbol{x}}},
\end{equation}

where $n_{\boldsymbol{x}}$ is the frequency of score pattern $\boldsymbol{x}$. Maximisation of $\mathcal{L}(\boldsymbol{\nu})$ would then lead to the maximum likelihood estimates of the item parameters $\boldsymbol{\hat{\nu}}$. This is generally how in IRT a model is fitted to data. \\
\indent After a model has been fitted as described above, the next step is to test how well the model fits the data. There are multiple options to assess model fit. Usually, both goodness-of-fit tests and fit indices are used. Common goodness-of-fit tests are a $\chi^2$-difference test and Pearson's $\chi^2$ test. The $\chi^2$-difference test uses the following LR statistic:

\begin{equation}
- 2 \ln \left \{ \frac{\max(\mathcal{L}_0)}{\max(\mathcal{L}_a)} \right \},
\end{equation}

where $\mathcal{L}_0$ is the likelihood of the model you fit (i.e., the null model) and $\mathcal{L}_a$ is the likelihood of an alternative model under which the null model is nested. According to Willks' theorem (Wilks, 1938), this LR statistic will asymptotically follow a $\chi^2$ distribution as sample size increases under the null hypothesis. Alternatively, the observed value of Pearson's $\chi^2$ statistic is:

\begin{equation}
\sum_{\boldsymbol{x}} \frac{(n_{\boldsymbol{x}} - \varepsilon_{\boldsymbol{x}})^2}{\varepsilon_{\boldsymbol{x}}},
\end{equation}

where $n_{\boldsymbol{x}}$ is the observed frequency for score pattern $\boldsymbol{x}$ and $\varepsilon_{\boldsymbol{x}}$ is the expected frequency for score pattern $\boldsymbol{x}$ given a model. This $\chi^2$ statistic also asymptotically follows a $\chi^2$ distribution as sample size approaches infinity. The MG LR test that we develop and evaluate in the present study is based on the LR statistic: 

\begin{equation}
-2 \ln \left \{ \frac{\max(\mathcal{L}_0)}{\prod_{j = 1}^g \max(\mathcal{L}_j)} \right \},
\end{equation}
 
where $\mathcal{L}_0$ is the likelihood of the chosen model for the whole dataset and $\mathcal{L}_j$ is the likelihood of the chosen model for group $j$. Group $j$ can be an existing group, or the groups can be gained by randomly assigning observations to $g$ groups. Wilks' theorem also applies to this LR statistic. Because all three aforementioned tests asymptotically follow a $\chi^2$ distribution under the null hypothesis, they can be used to assess goodness-of-fit. \\
\indent For fit indices, we research the performance of the TLI, CFI and ICFI. The TLI and CFI are estimated by comparing the tested model to a baseline and saturated model. Our baseline model is a complete-independence model with the following IRF:

\begin{equation}
P(X_i = 1 | \beta_{i}) = \frac{e^{- \beta_{i}}}{1 + e^{- \beta_{i}}},
\end{equation}

where the probability of scoring a 1 on item $i$ is dependent only on the difficulty of the item and no longer on a latent trait. We argue that this is an appropriate baseline model, because the IRF entails that the joint probability distribution is simply the product of the marginal probability distributions and the items are independent. In this baseline model, the probability of scoring a 1 on item $i$ is simply the proportion of people who score a 1 on item $i$. From this, it follows that the maximum likelihood estimate of $\beta_{i}$ is:

\begin{equation*}
\hat{\beta_{i}} = ln(\frac{n - n_i}{n_i}), 
\end{equation*}

where $n_i$ is the number of observations who scored a 1 on item $i$ and $n$ is the total number of observations. In a saturated model there are as many parameters as data points, which leads to a perfect fit. The saturated model is:

\begin{equation}
P(\boldsymbol{X} = \boldsymbol{x}) = \pi_{\boldsymbol{x}},
\end{equation}

where there no longer is an IRF. Instead, perfect model fit is gained by allowing each score pattern to have its own parameter ($\pi_{\boldsymbol{x}}$). The maximum likelihood estimate of $\pi_{\boldsymbol{x}}$, denoted by $\hat{\pi}_{\boldsymbol{x}}$, is then the relative frequency of the score pattern:

\begin{equation*}
\hat{\pi}_{\boldsymbol{x}} = \frac{n_{\boldsymbol{x}}}{n},
\end{equation*}

where $n_{\boldsymbol{x}}$ is the number of observations with score pattern $\boldsymbol{x}$ and $n$ is the total number of observations. With the baseline and saturated model, the TLI and CFI can be calculated through the following formulae:

\begin{equation}
\text{CFI} = 1 - \frac{\text{max}\{(\chi^2_T - df_T), 0\}}{\text{max}\{(\chi^2_T - df_T), (\chi^2_0 - df_0), 0\}},
\end{equation}
\begin{equation}
\text{TLI} = \frac{\chi^2_0/df_0 - \chi^2_T/df_T}{\chi^2_0/df_0 - 1}.
\end{equation}

In both equations, $\chi^{2}_{T}$ is the observed value of the LR statistic of a $\chi^2$-difference test between the tested model and the saturated model with $df_T$ degrees of freedom, and $\chi^{2}_{0}$ is the observed value of the LR statistic of a $\chi^2$-difference test between the baseline model and the saturated model with $df_0$ degrees of freedom. We also research the performance of the ICFI. Parallel to the Pearson's $\chi^2$ test, the ICFI gauges model fit by comparing the observed score pattern frequencies to the expected score pattern frequencies given a model:

\begin{equation}
\text{ICFI} = \frac{2 \cdot \sum_{\boldsymbol{x}}  n_{\boldsymbol{x}} \cdot \varepsilon_{\boldsymbol{x}}  }{  \sum_{\boldsymbol{x}}  {n_{\boldsymbol{x}}}^2 +  \sum_{\boldsymbol{x}}  {\varepsilon_{\boldsymbol{x}}} ^2 },
\end{equation}

where $n_{\boldsymbol{x}}$ is the observed frequency for score pattern $\boldsymbol{x}$ and $\varepsilon_{\boldsymbol{x}}$ is the expected frequency for score pattern $\boldsymbol{x}$. 

\section{Simulation study I}

In this simulation study, we consider dichotomous IRT with a unidimensional latent trait. For the MG LR test, we investigate its asymptotic properties under the null hypothesis and at which rate the test is able to reject a misspecified model under different conditions when groups are made through random assignment. We then compare this rate against the rejection rates of a $\chi^2$-difference and Pearson's $\chi^2$ test. For the TLI, CFI and ICFI, we investigate their average values and variance under correct and incorrect model specification. The simulation study was conducted in R (R Core Team, 2022).
\subsection{Data generation}
Data are generated by first sampling the person parameters $\theta$ from a standard normal distribution. Then, either the 2PL or 3PL is chosen as basis for the data generation (see below). Item parameters are kept static over all conditions. For the difficulty parameter $\beta_i$, repetitions of the values [-1.0, -0.5, 0.0, 0.5, 1.0] are chosen. As for the discrimination parameter $\alpha_i$, we choose from the values [0.7, 0.85, 1.0, 1.15, 1.3]. To create a more realistic scenario, these two parameters are then matched with one another in order to make sure that for every item difficulty, there are low and high discriminating items. Finally, when the data generating model is the 3PL, we set $\gamma_i$ to 0.25 for each item. Then, probabilities are estimated for all items on a test, given $\theta$, the chosen model and its item parameters. By sampling from a binomial distribution with the estimated probabilities, a matrix of simulated responses to a dichotomous test is created.
\subsection{Simulation design}
We vary four factors. We examine three conditions of test length ($I = 5, 10, 20$), five conditions of sample size ($N = 200, 300, 500, 1000, 1500$), and two conditions for the model on which data generation is based (2PL, 3PL). This design results in a total of 30 conditions. Furthermore, within each of these conditions, we then vary the MG LR test based on the number of groups ($G = 2, 3, 4$). We replicate each simulation condition 300 times. In each replication of each condition, we fit the 2PL. The 2PL is fitted using functions from the \textit{ltm} package (Rizopoulos, 2006), which approximates marginal maximum likelihood estimation through the Gauss-Hermite quadrature rule. Then we assess the performance of the three fit indices and the three different types of goodness-of fit tests: (1) a $\chi^2$-difference test, obtained by testing the 2PL under the 3PL with the constraint that all $\gamma_i$ have to be equal through the \textit{mirt} package (Chalmers, 2012), (2) Pearson's $\chi^2$ test, estimated through aggregating score patterns from the data and comparing the observed score pattern frequencies to the expected score pattern frequencies given the fitted 2PL, and (3) the MG LR test with a varying number of randomly assigned groups. 
\subsection{Performance metrics}
We study performance for the different goodness-of-fit tests by estimating both empirical $\alpha$ and power. Empirical $\alpha$ and power are estimated by obtaining the detection rate, which is the number of times the null hypothesis is rejected divided by the total number of replications per condition. Empirical $\alpha$ is estimated when generating data under the 2PL and fitting the 2PL. Power is estimated when generating data under the 3PL and fitting the 2PL. Then we compare the different type of tests with one another, where a test with lower power or higher empirical $\alpha$ is noted as performing worse. For all tests, we chose a level of significance of $\alpha = .05$. \\
\indent To measure the performance of the TLI, CFI and ICFI, we estimate their mean and standard error (SE) in each condition. Then, we can inspect whether the average fit index values decrease in the conditions where data are generated under the 3PL, compared to when data are generated under the 2PL. 

\subsection{Results}

To assess the performance of the MG LR test with randomised groups and the TLI, CFI and ICFI, we conducted the above described simulation study. Non-convergence of the models occurred $<.001$ percent of the time. When test length was 20, the empirical $\alpha$ and power estimates for Pearson's $\chi^2$ test were not estimated. This is because of the large amount of possible score patterns ($2^{20}$) in these conditions, which required too much computational power for the scope of the current study. Furthermore, negative values sometimes occurred for the TLI at higher test lengths under model misspecification. Previous literature has shown that this can occur when the degrees of freedom of the hypothesised model are small and correlations among observed variables are low (Wang \& Wang, 2019; Widaman \& Thompson, 2003). We rounded the average to 0 if it was negative.\\
\indent \textit{Table \ref{tab:1}} presents the empirical $\alpha$ estimates for the different $\chi^2$-based goodness-of-fit tests. Here, the asymptotic property of the tests can be observed clearly. As sample size increases, empirical $\alpha$ approximations approach the nominal level of .05. At sample sizes larger than 1000, the rejection rates of the MG LR and $\chi^2$-difference test seem to not deviate from the nominal level for different levels of test lengths. However, this means that for low sample sizes, the LR statistic of the MG LR test does not yet completely follow a $\chi^2$ distribution, resulting in higher rejection rates. For Pearson's $\chi^2$ test, the table shows that as the number of items increases, the empirical $\alpha$ increases dramatically from 0.06 to 0.22. This is in accordance with the issues we discussed in the introduction, where Pearson's $\chi^2$ test does not follow a $\chi^2$ distribution when many score patterns are low or missing. These results can be seen as confirmation for the theory and design behind our study. Important to remember, however, is that empirical data is almost never as clean.
% Table for Empirical Alpha results
\begin{table}[t!]
\caption{Empirical $\alpha$ estimates for the different goodness-of-fit tests}
\begin{tabular}{ r r | a c a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{5}{c}{Goodness-of-fit test} \\
\rowcolor{white} \textit{I} & \textit{N} & LR2 & LR3 & LR4 & $\Delta\chi^2$ & P-$\chi^2$ \\
\midrule
5 & 200 & 0.09 & 0.06 & 0.10 & 0.05 & 0.06 \\ 
& 300 & 0.07 & 0.08 & 0.07 & 0.04 & 0.04 \\
& 500 & 0.07 & 0.07 & 0.06 & 0.03 & 0.05 \\
& 1000 & 0.04 & 0.06 & 0.07 & 0.03 & 0.06 \\
& 1500 & 0.07 & 0.07 & 0.06 & 0.02 & 0.04 \\
10 & 200 & 0.08 & 0.11 & 0.11 & 0.03 &  0.22 \\ 
& 300 & 0.04 & 0.05 & 0.09 & 0.04 & 0.20 \\
& 500 & 0.04 & 0.05 & 0.06 & 0.06 & 0.17 \\
& 1000 & 0.02 & 0.06 & 0.06 & 0.03 & 0.12 \\
& 1500 & 0.03 & 0.05 & 0.04 & 0.02 & 0.11 \\
20 & 200 & 0.07 & 0.08 & 0.12 & 0.06 & -- \\ 
& 300 & 0.05 & 0.08 & 0.10 & 0.04 & -- \\
& 500 & 0.07 & 0.08 & 0.06 & 0.03 & -- \\
& 1000 & 0.05 & 0.05 & 0.05 & 0.02 & -- \\
& 1500 & 0.04 & 0.05 & 0.05 & 0.03 & -- \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = two-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; LR2 = MG LR test with $g = 2$; LR3 = MG LR test with $g = 3$; LR4 = MG LR test with $g = 4$; $\Delta\chi^2$ = $\chi^2$-difference test under the three-parameter logistic model with equal $\gamma_i$ constraint; P-$\chi^2$ = Pearson's $\chi^2$ test.
\label{tab:1}
\end{table}

\indent Next, we present the power estimates of the different goodness-of-fit tests in \textit{Table \ref{tab:2}}. For the $\chi^2$-difference test, the table presents clearly that as sample size and/or test length increase, the power to reject the tested model also increases in favour of the true model up to a power of 1.00 when $I = 20$ \& $N = 1500$. The results for Pearson's $\chi^2$ test reflect that the test seems to not be able to detect any model misspecification with power estimates only slightly larger than the empirical $\alpha$ estimates. For the MG LR test, the table shows a very interesting result. Namely, the power estimates of the test still converge to the nominal $\alpha$ level of .05. This means that when not under the null hypothesis, the LR statistic of the MG LR test still asymptotically follows a $\chi^2$ distribution when groups are based on random assignment. This result indicates that the test cannot be used as a goodness-of-fit test to detect model misspecification under randomised groups. Moreover, the result also implies that the test is insensitive to violations of correct model specification in testing for group differences. Therefore, we explore how effective the test is at detecting group differences and the effect of incorrect model specification on detecting group differences in a follow-up simulation study.

\begin{table}[ht]
\caption{Power estimates for the different goodness-of-fit tests}
\begin{tabular}{ r r | a c a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{5}{c}{Goodness-of-fit test} \\
\rowcolor{white} \textit{I} & \textit{N} & LR2 & LR3 & LR4 & $\Delta\chi^2$ & P-$\chi^2$ \\
\midrule
5 & 200 & 0.10 & 0.09 & 0.12 & 0.06 & 0.06 \\ 
& 300 & 0.09 & 0.08 & 0.14 & 0.08 & 0.06 \\
& 500 & 0.07 & 0.09 & 0.12 & 0.11 & 0.04 \\
& 1000 & 0.06 & 0.05 & 0.08 & 0.12 & 0.07 \\
& 1500 & 0.05 & 0.07 & 0.05 & 0.19 & 0.07 \\
10 & 200 & 0.09 & 0.14 & 0.20 & 0.25 & 0.29 \\ 
& 300 & 0.11 & 0.11 & 0.13 & 0.26 & 0.21 \\
& 500 & 0.05 & 0.09 & 0.08 & 0.32 & 0.22 \\
& 1000 & 0.08 & 0.07 & 0.07 & 0.58 & 0.16 \\
& 1500 & 0.06 & 0.06 & 0.06 & 0.75 & 0.22 \\
20 & 200 & 0.10 & 0.11 & 0.16 & 0.52 & -- \\ 
& 300 & 0.08 & 0.10 & 0.11 & 0.66 & -- \\
& 500 & 0.05 & 0.08 & 0.08 & 0.89 & -- \\
& 1000 & 0.03 & 0.05 & 0.06 & 0.97 & -- \\
& 1500 & 0.03 & 0.05 & 0.07 & 1.00 & -- \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = three-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; LR2 = MG LR test with $g = 2$; LR3 = MG LR test with $g = 3$; LR4 = MG LR test with $g = 4$; $\Delta\chi^2$ = $\chi^2$-difference test under the three-parameter logistic model with equal $\gamma_i$ constraint; P-$\chi^2$ = Pearson's $\chi^2$ test.
\label{tab:2}
\end{table}

\indent For the fit indices, \textit{Table \ref{tab:3}} and \textit{Table \ref{tab:4}} exhibit the means and standard errors for the TLI, CFI and ICFI under correct and incorrect model specification respectively. The tables show that as sample size increases, the fit index values move closer to 1 (i.e., a good model fit). However, if test length increases, fit index values move closer to 0 instead. This can be interpreted in the following way: as test length increases, the fit of the baseline model becomes relatively better compared to the tested model. This result has not been found before in IRT literature. Nonetheless, even though the fit indices tend to 0 as test length increases, the means for the TLI and CFI are higher under correct model specification compared to incorrect model specification for every single condition (e.g., for $N = 400$ \& $I = 10$, the mean of the TLI is 0.11 under correct model specification, whereas it is .00 under incorrect model specification). This means that the TLI and CFI can be used for model fit decisions when the 2PL is fit to the data. However, the tables show that for cases where there is a small sample size and few test items, there are high standard errors for the TLI and CFI. This indicates that when assessing model fit through use of the TLI and CFI in low sample size settings, point estimates might not be good enough. Other methods such as bootstrapping might be required in order to be able to properly decide which model fits best. For high sample sizes and/or larger test lengths, this should not pose an issue. \\

\begin{table}[ht!]
\caption{TLI, CFI and ICFI values under correct model specification}
\begin{tabular}{ r r | a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{1}{c}{TLI} & \multicolumn{1}{c}{CFI} & \multicolumn{1}{c}{ICFI} \\
\rowcolor{white} \textit{I} & \textit{N} & M (SE) & M (SE) & M (SE) \\
\midrule
 5 & 200 & 0.63 (0.19) & 0.75 (0.12) & 0.98 (0.01) \\
& 300 & 0.74 (0.14) & 0.82 (0.09) & 0.98 (0.01) \\
& 500 & 0.84 (0.09) & 0.90 (0.06) & 0.99 (0.00) \\
& 1000 & 0.92 (0.05) & 0.94 (0.03) & 0.99 (0.00) \\
& 1500 & 0.95 (0.03) & 0.96 (0.02) & 1.00 (0.00) \\
10 & 200 & 0.06 (0.06) & 0.23 (0.05) & 0.69 (0.04) \\
& 300 & 0.11 (0.05) & 0.27 (0.04) & 0.77 (0.03) \\
& 500 & 0.19 (0.04) & 0.33 (0.04) & 0.85 (0.02) \\
& 1000 & 0.32 (0.03) & 0.45 (0.03) & 0.91 (0.01) \\
& 1500 & 0.42 (0.03) & 0.53 (0.03) & 0.94 (0.01)\\
20 & 200 & 0.04 (0.02) & 0.14 (0.02) & 0.03 (0.01) \\
& 300 & 0.05 (0.02) & 0.14 (0.02) & 0.04 (0.02) \\
& 500 & 0.05 (0.01) & 0.14 (0.01) & 0.06 (0.02) \\
& 1000 & 0.07 (0.01) & 0.16 (0.01) & 0.11 (0.02) \\
& 1500 & 0.08 (0.01) & 0.17 (0.01) & 0.17 (0.03) \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = two-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; TLI = tucker-lewis index; CFI = comparative fit index; ICFI = identity coefficient fit index; M = mean; SE = standard error.
\label{tab:3}
\end{table}

\indent The ICFI shows less promise compared to the TLI and CFI in its ability to determine model fit. The means seem to not differ in incorrect model specification compared to correct model specification. In fact, the means are usually slightly higher under incorrect model specification. Therefore, we argue that the ICFI is unable to determine whether there was model misspecification. This finding corresponds with the results given by the Pearson's $\chi^2$ test. Together, they strongly indicate that model fit measures based on observed and expected score-pattern frequencies are not able to detect model misspecification when working with the 2PL. \\

\begin{table}[t!]
\caption{TLI, CFI and ICFI values under incorrect model specification}
\begin{tabular}{ r r | a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{1}{c}{TLI} & \multicolumn{1}{c}{CFI} & \multicolumn{1}{c}{ICFI} \\
\rowcolor{white} \textit{I} & \textit{N} & M (SE) & M (SE) & M (SE) \\
\midrule
 5 & 200 & 0.33 (0.31) & 0.55 (0.19) & 0.98 (0.01) \\
& 300 & 0.49 (0.24) & 0.66 (0.16) & 0.99 (0.00) \\
& 500 & 0.66 (0.18) & 0.77 (0.12) & 0.99 (0.00) \\
& 1000 & 0.81 (0.11) & 0.87 (0.07) & 1.00 (0.00) \\
& 1500 & 0.86 (0.08) & 0.91 (0.05) & 1.00 (0.00) \\
10 & 200 & 0.00 (0.04) & 0.11 (0.03) & 0.76 (0.05) \\
& 300 & 0.00 (0.04) & 0.13 (0.03) & 0.83 (0.03) \\
& 500 & 0.00 (0.03) & 0.17 (0.03) & 0.89 (0.02) \\
& 1000 & 0.07 (0.03) & 0.24 (0.02) & 0.94 (0.01) \\
& 1500 & 0.15 (0.03) & 0.31 (0.02) & 0.96 (0.01) \\
20 & 200 & 0.00 (0.01) & 0.06 (0.01) & 0.04 (0.02) \\
& 300 & 0.00 (0.01) & 0.06 (0.01) & 0.06 (0.03) \\
& 500 & 0.00 (0.01) & 0.07 (0.01) & 0.10 (0.03) \\
& 1000 & 0.00 (0.01) & 0.07 (0.01) & 0.17 (0.03) \\
& 1500 & 0.00 (0.01) & 0.08 (0.01) & 0.24 (0.03) \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = three-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; TLI = tucker-lewis index; CFI = comparative fit index; ICFI = identity coefficient fit index; M = mean; SE = standard error.
\label{tab:4}
\end{table}

\section{Simulation Study II}
Due to the result we found in Simulation Study I, where under model misspecification the MG LR test still asymptotically follows a $\chi^2$ distribution, we were inspired to run this follow-up simulation study. Here, we investigate how effective the MG LR test is at detecting different types of group differences under correct and incorrect model specification for different test lengths and sample sizes. Besides general effectiveness, we are interested in ascertaining whether there is a difference in effectiveness of the MG LR test to detect group differences when an incorrect model is fitted to the data compared to the correct model. We again only consider IRT with unidimensionality and dichotomous test items. The simulation study was conducted in R. \\
\indent Similar to Simulation Study I, we again vary the four factors test length ($I = 5, 10, 20$), sample size ($N = 200, 300, 500, 1000, 1500$), the model type (2PL, 3PL) and the number of groups ($G = 2, 3, 4$). We also investigate a fifth factor: group differences, which has three conditions, (1) differences in $\theta$, (2) differences in the item parameters $\alpha_i$ and $\beta_i$ and (3) differences in the item parameters $\alpha_i$, $\beta_i$ and $\gamma_i$ (only applicable to the 3PL condition). This results in a total of 225 distinct rejection rates. Each condition is replicated 300 times. \\
\indent Data generation for Simulation Study II works in the same way as in Simulation Study I with one key difference. Now, data generation is no longer according to the same parameters described in Simulation Study I for the whole dataset. Instead, the parameters are dependent on two factors: the group differences factor, and the number of groups factor $G$. In the differences in $\theta$ conditions, the item parameters are the same for all $G$ groups. However, the mean of the normal distribution for $\theta$ depends on group membership. For each consecutive every group past the first group, there is a mean difference of $-0.25$ with the previous group. For example, when $G = 3$, group one has a mean of 0, group two a mean of $-0.25$, and group three a mean of $-0.50$. In the differences in item parameter conditions, the opposite happens. Here, the person parameters are all sampled from the same standard normal distribution. However, the item parameters now depend on group membership. For the discrimination parameter per item, $\alpha_i$, all initial values (identical to Simulation Study I) are lowered by 0.10 for each consecutive group. Whereas for the difficulty parameter per item, $\beta_i$, the values are lowered by 0.25 per consecutive group. To illustrate, when $G = 2$, group one has $\alpha_1 = 0.70$ and $\beta_1 = -1$, and group two has $\alpha_1 = 0.60$ and $\beta_1 = -1.25$. For the conditions where $\gamma_i$ differs, we chose to lower the $\gamma_i$ values by .05 per consecutive group. Finally, we no longer estimate both the $\chi^2$-difference and Pearson's $\chi^2$ test, as these are used to detect model misspecification, whereas we are now interested in using the MG LR test to detect group differences. Performance is again measured by the detection rate. However, now they are all considered power estimates against group differences.

\subsection{Results}

To research whether the MG LR test has power to discern whether there are group differences and the effect incorrect model specification has on the power of the test, we ran the above described simulation study where data is generated according to different types of group differences. Non-convergence of the models occurred $<.001$ percent of the time.

\begin{table}[t!]
\caption{Power estimates for the MG LR test under group differences in $\theta$}
\begin{tabular}{ r r | a c a | c a c }
\toprule
\multicolumn{2}{c}{} & \multicolumn{6}{c}{Data Generating Model} \\
\multicolumn{2}{c}{} & \multicolumn{3}{c}{2PL} & \multicolumn{3}{c}{3PL} \\
\multicolumn{2}{c}{Conditions} & \multicolumn{3}{c}{\textit{G}} & \multicolumn{3}{c}{\textit{G}} \\
\rowcolor{white} \textit{I} & \textit{N} & 2 & 3 & 4 & 2 & 3 & 4 \\
\midrule
5 & 200 & 0.13 & 0.28 & 0.37 & 0.15 & 0.24 & 0.31 \\ 
& 300 & 0.15 & 0.32 & 0.46 & 0.14 & 0.23 & 0.41 \\
& 500 & 0.20 & 0.42 & 0.71 & 0.17 & 0.36 & 0.59 \\
& 1000 & 0.40 & 0.79 & 0.98 & 0.33 & 0.64 & 0.90 \\
& 1500 & 0.65 & 0.96 & 1.00 & 0.47 & 0.85 & 0.98 \\
10 & 200 & 0.13 & 0.26 & 0.40 & 0.18 & 0.23 & 0.44 \\ 
& 300 & 0.17 & 0.24 & 0.44 & 0.15 & 0.30 & 0.50 \\
& 500 & 0.18 & 0.43 & 0.72 & 0.19 & 0.34 & 0.63 \\
& 1000 & 0.42 & 0.80 & 0.98 & 0.38 & 0.66 & 0.92 \\
& 1500 & 0.62 & 0.97 & 1.00 & 0.49 & 0.89 & 1.00 \\
20 & 200 & 0.13 & 0.23 & 0.35 & 0.10 & 0.25 & 0.39 \\ 
& 300 & 0.14 & 0.23 & 0.41 & 0.13 & 0.26 & 0.37 \\
& 500 & 0.17 & 0.40 & 0.65 & 0.15 & 0.36 & 0.57 \\
& 1000 & 0.38 & 0.76 & 0.97 & 0.26 & 0.68 & 0.89 \\
& 1500 & 0.57 & 0.94 & 1.00 & 0.48 & 0.88 & 1.00 \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; 2PL = two-parameter logistic model; 3PL = three-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; \textit{G} = number of groups under which the data was generated.
\label{tab:5}
\end{table}

\indent For an average differences of 0.25 in $\theta$ between each consecutive group, \textit{Table \ref{tab:5}} presents the power of the MG LR test under the different conditions. The table shows exactly what one would expect of a power study, where the power to reject the null hypothesis approaches 1.00 as sample size increases. The table also shows a main effect of the number of groups, where the power to reject the null hypothesis increases as the number of groups increases. The power estimates are slightly lower in conditions when the data is generated under the 3PL. This means that incorrect model specification leads to a small loss in power. Finally, the table shows no main effect of test length on the power of the MG LR test to detect group differences. \\

\newpage

\begin{table}[ht!]
\caption{Power estimates for the MG LR test under group differences in item parameters}
\begin{tabular}{ r r | a c a | c a c | a c a }
\toprule
\multicolumn{2}{c}{} & \multicolumn{9}{c}{Data Generating Model} \\
\multicolumn{2}{c}{} & \multicolumn{3}{c}{2PL} & \multicolumn{3}{c}{3PL} & \multicolumn{3}{c}{3PL $( \gamma_i ) $}\\
\multicolumn{2}{c}{Conditions} & \multicolumn{3}{c}{\textit{G}} & \multicolumn{3}{c}{\textit{G}} & \multicolumn{3}{c}{\textit{G}} \\
\rowcolor{white} \textit{I} & \textit{N} & 2 & 3 & 4 & 2 & 3 & 4 & 2 & 3 & 4 \\
\midrule
5 & 200 & 0.12 & 0.31 & 0.36 & 0.15 & 0.24 & 0.30 & 0.19 & 0.46 & 0.57 \\ 
& 300 & 0.20 & 0.34 & 0.43 & 0.14 & 0.23 & 0.39 & 0.28 & 0.51 & 0.80 \\
& 500 & 0.22 & 0.43 & 0.73 & 0.18 & 0.38 & 0.56 & 0.41 & 0.80 & 0.96 \\
& 1000 & 0.46 & 0.82 & 0.98 & 0.33 & 0.68 & 0.86 & 0.76 & 1.00 & 1.00 \\
& 1500 & 0.72 & 0.98 & 1.00 & 0.51 & 0.85 & 0.98 & 0.93 & 1.00 & 1.00 \\
10 & 200 & 0.14 & 0.30 & 0.48 & 0.19 & 0.31 & 0.50 & 0.31 & 0.49 & 0.75 \\ 
& 300 & 0.21 & 0.30 & 0.51 & 0.18 & 0.35 & 0.62 & 0.36 & 0.64 & 0.88 \\
& 500 & 0.22 & 0.45 & 0.80 & 0.21 & 0.39 & 0.65 & 0.49 & 0.85 & 0.99 \\
& 1000 & 0.53 & 0.88 & 0.98 & 0.46 & 0.76 & 0.93 & 0.87 & 1.00 & 1.00 \\
& 1500 & 0.76 & 0.98 & 1.00 & 0.63 & 0.90 & 1.00 & 0.99 & 1.00 & 1.00 \\
20 & 200 & 0.13 & 0.23 & 0.35 & 0.10 & 0.25 & 0.39 & 0.21 & 0.54 & 0.75 \\ 
& 300 & 0.14 & 0.23 & 0.41 & 0.13 & 0.26 & 0.37 & 0.32 & 0.63 & 0.89 \\
& 500 & 0.17 & 0.40 & 0.65 & 0.15 & 0.36 & 0.57 & 0.46 & 0.87 & 1.00 \\
& 1000 & 0.38 & 0.76 & 0.97 & 0.26 & 0.68 & 0.89 & 0.87 & 1.00 & 1.00 \\
& 1500 & 0.57 & 0.94 & 1.00 & 0.48 & 0.88 & 1.00 & 0.97 & 1.00 & 1.00 \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; 2PL = two-parameter logistic model.; 3PL = three-parameter logistic model where only $\alpha_i$ and $\beta_i$ differ per group; 3PL $ (\gamma_i)$: three-parameter logistic model where $\alpha_i$, $\beta_i$ and $\gamma_i$ differ per group; \textit{I} = test length; \textit{N} = sample size; \textit{G} = number of groups under which the data was generated.
\label{tab:6}
\end{table}

\indent \textit{Table \ref{tab:6}} presents the power of the MG LR test when differences are based on the item parameters. Similar results can be found here, where again power increases as either sample size or the number of groups increases. There is also no main effect of test length on power. Incorrect model specification again leads to a slight decrease in power estimates compared to correct model specification. However, looking at the conditions where the $\gamma_i$ parameters change per group, we find that the power values are much higher compared to their corresponding conditions when only $\alpha_i$ and $\beta_i$ parameters change per group. This entails that the power of the MG LR test is dependent on the amount of group differences in item parameters, even if these group differences cannot properly be reflected in the fitted model. The overarching results are that the MG LR test seems to be very perceptive to group differences in either the distribution of the latent trait or item parameters, especially with sample sizes larger than 500 and more than two groups and that incorrect model specification leads to only a slight loss in power. \\

\section{Empirical example}
To demonstrate the possible uses the TLI and CFI in dichotomous IRT, we estimated and interpreted their values on Section 6 of the Law School Admission Test (LSAT) dataset, given by Bock \& Lieberman (1970). The dataset contains 1000 observations on 5 homogenous Figure Classification items on the unidimensional subject of Debate. The items were multiple choice, where a 1 denoted a correct answer, whereas a 0 denoted a wrong answer. We fitted four models in total: the 2PL, the 3PL with no constraints, the 3PL with the constraint that all $\gamma_i$ have to be equal and the 3PL where we set all $\gamma_i$ to 0.25. Afterwards, we evaluated the appropriateness of each models by calculating the TLI and CFI. For the 2PL, the TLI equaled 0.57 and the CFI equaled 0.81. For the 3PL with no constraints, the TLI lowered to 0.09 and the CFI lowered 0.70. For the 3PL with equal $\gamma_i$, the TLI rose to 0.53 and the CFI rose to 0.79. Finally, for the 3PL where we set all $\gamma_i$ to 0.25, the TLI was 0.56 and the CFI was 0.80. \\
\indent From these results, we conclude the following: allowing each item to have a free $\gamma_i$ leads to a worse fit according to the TLI and CFI. However, setting the constraint that the pseudo-guessing parameters have to be equal or setting them to a certain value does not lead to a much worse fit compared to the 2PL according to the TLI and CFI. This is aligned with the fact that the test consisted of multiple choice items, which means that all items do have an baseline probability of being scored correctly. 

\section{Discussion}

The current study investigated existing and new measures of model fit to test for model misspecification and group differences in IRT. Through the first simulation study, we observed that the proposed MG LR test requires sample sizes larger than 1000 in order to properly follow a $\chi^2$ distribution. The MG LR test, however, was not able to detect any model misspecification under random group assignment. Rather, under incorrect model specification the MG LR test still asymptotically followed a $\chi^2$ distribution. Similarly, we observed that model fit measures that asses goodness-of-fit through estimating the difference between expected and observed score pattern frequencies (i.e., the ICFI and Pearson's $\chi^2$ test) had no power to detect model misspecification when fitting the 2PL. \\
\indent Contrary to these results, we found that the TLI and CFI were very sensitive to detecting model misspecification in dichotomous IRT. This is an important result considering the results given by Nye et al. (2020), who found few fit indices that were sensitive to these types of misspecification. However, against the expectations of Cai et al. (2021), who speculated that the TLI would not depend heavily on model size (i.e., test length), we discovered that the TLI and CFI estimates lower substantially as test length increases. The phenomenon might be due to our shared choice of baseline model. Previous research by Widaman \& Thompson (2003) and Van Laar \& Braeken (2021) has already discussed the importance of baseline models for incremental fit indices. Future research could look into the relationship between baseline model, test length and TLI/CFI values to a greater extent. We must also remember that compared to Cai et al., we used a different statistic for the calculation of the fit indices which could have affected the results. Fortunately, the effect of test length does not affect the fit indices' sensitivity to model misfit. It only poses issues for any rule-of-thumbs associated with the usage of the TLI and CFI. Cai et al. further speculated that the CFI would not have a similar interpretation in IRT as it does in SEM literature. However, our results indicate that the behaviour of the CFI follows the same trend as the TLI in IRT. We strongly recommend that future studies in IRT incorporate the TLI and CFI in evaluating the appropriateness of different models. We showcased an example of how the TLI and CFI can add additional insights for model evaluation in IRT. \\
\indent Further exploring possible uses of the MG LR test, a follow-up simulation study showed that the MG LR test is effective at detecting group differences. The test is able to detect small group differences in the population mean - according to Cohen's (1988) standards for effect size in behavioural sciences - above the 0.80 power threshold once sample size rose above 1000 with more than two groups. Taken together, it means that the MG LR test could be a useful addition to the multi-group analysis literature. This is because the MG LR test tests only for group differences and mostly ignores model specification, where incorrect model specification leads only to a small loss in power. % nog kort zeggen dat model niet alle differences hoeft te reflecten properly?
The largest limitation of the MG LR test is that it is unable to determine where these group differences lie. For this, a researcher has to investigate the latent construct and the items of the questionnaire through Measurement Invariance techniques. \\
\indent Naturally, we also have to bear in mind the context of the present study. We studied these different measures of model fit and group differences solely in IRT with dichotomous items and unidimensional latent traits. For the MG LR test, this signifies that we do not know whether the test is also sensitive for group differences beyond this setting. Future work could further investigate the asymptotic properties and power of the test for group differences in polytomous and/or multidimensional IRT and continuous settings such as in SEM. Furthermore, as briefly mentioned before, we worked with simulated data where we made sure that no assumptions were violated. In real-life settings, this may almost never happen and this should be taken into account with usage of the measures reported here. \\ % nog recommendatie voor toekomstig onderzoek hierbij?
\indent To summarise, preliminary evidence brought about our belief that the TLI and CFI show promise as valuable aids for model evaluation in dichotomous IRT research. On the contrary, the ICFI, Pearson's $\chi^2$ test and the MG LR test do not seem to be able to help in this research setting. However, when researching differences between groups in unidimensional dichotomous IRT, the MG LR test can be used to detect whether these differences exist while disregarding model specification. Model evaluation in IRT remains an issue that requires more light to be shed upon in future research. Hopefully, the conducted research here is able to improve model evaluation ever so slightly in the social and behavioural sciences. 

\begin{sm}
All annotated code and results used in the current study can be found online on the Open Science Framework by clicking \href{https://osf.io/dtbcr/?view_only=e32e5f8a43124434b5a53a44ff26ad23}{here}.
\end{sm}

\newpage

\nocite{*}
\bibliographystyle{apalike}
\bibliography{reportref}

\end{document}
