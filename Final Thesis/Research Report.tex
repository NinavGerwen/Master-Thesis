% setting document class according to SAGE guidelines
\documentclass[Royal,sageapa,times,doublespace]{sagej}

% packages for APA style requirements according to SAGE guidelines
\usepackage{moreverb,url}
\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}
\usepackage[compact]{titlesec}

\titlespacing{\section}{0pt}{*0}{*1}


% mathematical (equation) packages
\usepackage{amsmath}
\usepackage{amssymb}

% table-related packages for APA style tables
\usepackage{multirow,booktabs,setspace,caption}
\usepackage{tikz}
\usepackage{xcolor, colortbl}

% some document-wide table settings to be able to use
\definecolor{Gray}{gray}{0.90}
\newcolumntype{a}{>{\columncolor{Gray}}c}

% document settings according to SAGE guidelines
\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\volumeyear{2023}

% beginning the manuscript
\begin{document}

\runninghead{van Gerwen}% and Hessen}

\title{Assessing Fit of Item Response Theory Models using a Multi-Group Likelihood Ratio Test and Fit Indices}

\author{Nina van Gerwen \affilnum{1}} % and Dave Hessen\affilnum{2}}

\affiliation{\affilnum{1}Utrecht University, NL} % \\
% \affilnum{2}Utrecht University, NL}

\corrauth{Nina van Gerwen, Utrecht University,
Faculty of Social Sciences, Department of Methodology and Statistics,
Padualaan 14, Utrecht, 3584 CH, NL.}

\email{n.l.vangerwen@students.uu.nl}

\begin{abstract}
{\small
In Item Response Theory (IRT), it is important to investigate the appropriateness of the models you fit to the data to ensure valid measurements and conclusions. Model appropriateness is typically assessed through goodness-of-fit tests and fit indices. However, previous research has shown that for dichotomous IRT models, currently available goodness-of-fit tests and fit indices suffer from various issues. The present study builds upon previous research by investigating a new goodness-of-fit test based on multiple group analysis, the MG LR test, and three scarcely studied fit indices: (1) Tucker Lewis index (TLI), (2) Comparative Fit index (CFI) and (3) Identity Coefficient Fit index (ICFI). We examine the performance of these fit measures through a simulation study. The results suggests that the ICFI and the MG LR test are unable to determine model misspecification in dichotomous IRT. In contrast, the TLI and CFI performed well in determining model misspecification in this setting. A follow-up simulation study showed that the MG LR test is effective in detecting group differences, while insensitive to incorrect model specification. We believe this to be a unique feature of the test and speculate its value as an instrument in detecting group differences. We also recommend that the TLI and CFI become more standard in use in order to improve model appraisal in IRT. 
\par}
\end{abstract}

\keywords{\small{item response theory, goodness-of-fit tests, fit indices, power, group differences}}

\maketitle

\section{
\centering
Assessing Fit of Item Response Theory Models using a Multi-Group Likelihood Ratio Test and Fit Indices
}
Item Response Theory (IRT) is an often used tool for designing and analysing tests/questionnaires in psychological, educational and organisational practices. IRT models infer latent traits -- characteristics that are not directly observable (e.g., attitudes or intelligence) -- through analysing the answers to a test. The goal is to find the most parsimonious model that best describes the scores on the test items. To achieve this, the model must show a good fit. If a model is used that does not fit the data, it can lead to dire consequences such as faults in the validity of your measurement and by extension your conclusion (Cri\c{s}an et al., 2017; Jiao \& Lau, 2003; Zhao \& Hambleton, 2017). Therefore, model fit should be assessed after fitting an IRT model to test data. \\
\indent There are mainly two procedures that can be applied to quantify the appropriateness of a model. First, model fit can be assessed through goodness-of-fit tests. Goodness-of-fit tests are statistical hypothesis tests that describe how well the observed data follow the expected data given a model. \\
\indent Common goodness-of-fit tests in IRT are a $\chi^2$-difference test and Pearson's $\chi^2$ test. These tests suffer from issues, however. Pearson's $\chi^2$ test, for example, will not follow a $\chi^2$ distribution when many score patterns are missing or have a low frequency, which is often the case -- especially as test length increases. The $\chi^2$-difference test instead is difficult to use for two dichotomous IRT models: the two- (2PL) and three-parameter logistic (3PL) model. These models both specify the probability of scoring a dichotomous item correctly as a logistic function of a latent trait and two or three item characteristics. The issue with these models and their generalisations is that they suffer from consistency and estimation issues (Barton \& Lord, 1981; Loken \& Rulison, 2010). As a result, the $\chi^2$-difference test is hard to use for these models. Another concern with the tests is power, which can be viewed as both a blessing and a curse. This is because many goodness-of-fit tests suffer from a lack of power when sample size is low. However, when sample size increases, the power to reject any reasonable model that does not perfectly fit, also increases (Curran et al., 1996). These findings suggest that there is a lack of usable goodness-of-fit tests to test dichotomous IRT models. \\
\indent Therefore, the current study explores a new possible goodness-of-fit test to assess model misspecification in dichotomous IRT. The test we are interested in originates from multi-group analyses, where it can be used to detect group differences in existing groups. However, what if there were no existing groups, and groups are made by random assignment instead. In this scenario, if a wrong model is specified, would the test have power to reject this model in favour of the true model? We expect that the test will have power to detect this model misspecification. If the test does not turn out to have any power, it would mean that the test is insensitive to incorrect model specification. If the interest is then in testing for differences between existing groups, it means that the test ignores model misspecification and tests solely for group differences. \\
\indent The second procedure of assessing model fit is through fit indices. Fit indices are mathematical descriptives that indicate how close the fit of a model is to a perfect fit. Note, however, that fit indices alone are not sufficient to infer whether a model fits well, as they are not inferential statistics and only describe the overall fit. Taken together with goodness-of-fit tests, fit indices do allow researchers to have a more comprehensive overview of model fit. For example, when due to a lot of power, a goodness-of-fit test shows that the model should be rejected, fit indices can indicate whether the model is still reasonable. \\ 
\indent Previous research within IRT has seen the development of multiple fit indices (e.g., $Y\text{-}Q_1$; Yen, 1981, $\text{RMSEA}_n$; Maydeu-Olivares \& Joe, 2014). Nye et al. (2020) researched several IRT fit indices and found that many fit indices exhibit poor performance when the 2PL is fit to the data. Fit indices from Structural Equation Modeling (SEM) can also be used in IRT. Although the performance of multiple fit indices from SEM have been researched in IRT (e.g., RMSEA \& SRMR; Maydeu-Olivares \& Joe, 2014), hardly any past research investigated the Tucker-Lewis Index (TLI; Tucker \& Lewis, 1973) and Comparative Fit Index (CFI; Bentler, 1990). We found only one paper that examined the CFI (Yang, 2020) and two papers that investigated the TLI (Cai et al., 2021; Yang, 2020) in IRT. Furthermore, these papers came to different conclusions. Where Yang (2020) found that the CFI and TLI showed unsatisfactory performance in distinguishing data following the 2PL and 3PL, Cai et al. (2021) concluded that the TLI showed promise as a valuable index in evaluating IRT models. A possible reason for these mixed results could be due to differences in baseline model and calculations. Previous research has conveyed the importance of baseline models for incremental fit indices such as the TLI and CFI (Widaman \& Thompson, 2003; Van Laar \& Braeken; 2021). Since the calculations and baseline model influence the performance of the fit indices, we were inspired to research the TLI and CFI further in dichotomous IRT. We examine the TLI and CFI to a greater degree by investigating their performance while calculated through the $\chi^2$ statistic and with a baseline model that assumes independence between test items. \\
\indent Additionally, we research another possible fit index not investigated before in IRT. This fit index is based on the identity coefficient by Zegers \& ten Berge (1985), which indicates the degree to which two vectors are identical. Therefore, this coefficient can reflect the degree to which the observed score pattern frequencies follow the expected score pattern frequencies given a model. \\
\indent To summarise, in dichotomous IRT there are not enough measures of model fit available and scarce studies investigating the performance of the TLI and CFI. We address these two issues by developing and evaluating a new Likelihood Ratio (LR) goodness-of-fit test based on multi-group analysis, named the Multiple Group LR (MG LR) test. We also develop and evaluate a new fit index inspired by the identity coefficient by Zegers \& ten Berge (1985), named the identity coefficient fit index (ICFI). Furthermore, we assess the performance of the TLI and CFI based on new calculations. Specifically, we answer and discuss the following four research questions through multiple simulation studies and an empirical example: \\
\begin{enumerate}
\item{What sample size is necessary at different test lengths for the MG LR test to approximately follow a $\chi^2$ distribution?}
\item{What is the power of the MG LR test to detect model misspecification with groups based on randomised assignment when fitting the 2PL?}
\item{Under randomised group assignment, how does the performance of the MG LR test compare to the performance of a $\chi^2$-difference and Pearson's $\chi^2$ test in detecting model misspecification when working with the 2PL?}
\item{What is the performance of the TLI, CFI and ICFI in assessing model fit when the 2PL is fit to the data?}
\end{enumerate}

\section{\centering Methods}
In IRT, the goal is to find a model that best describes scores on test items. To achieve this, IRT presupposes three assumptions: (1) conditional independence of items given the latent trait, (2) independence of observations and (3) the response to an item can be modeled by an item response function (IRF). An IRF is a mathematical equation that relates the probability to score a certain category on an item to the latent trait. The IRF for the 3PL (Birnbaum, 1968) is:

\begin{equation}
P(X_i = 1 | \theta, \alpha_{i}, \beta_{i}, \gamma_{i}) = \gamma_{i} + (1 - \gamma_{i}) \cdot 
\frac{e^{\alpha_{i}\theta - \beta_{i}}}{1 + e^{\alpha_{i}\theta - \beta_{i}}},
\end{equation}

where $X_i$ is a random variable indicating the response to item $i$. The probability of scoring a 1 on item $i$ in the 3PL depends on (a) the latent trait $\theta$, (b) the location parameter of the item $\beta_{i}$, which denotes how difficult the item is, (c) the scaling parameter of the item $\alpha_{i}$, which signifies how well item $i$ discriminates between individuals with lower and higher $\theta$, and (d) an item-specific lower asymptote $\gamma_{i}$, which indicates whether there is a baseline probability of scoring a $1$ (e.g., a multiple choice question with 4 options has a $0.25$ baseline probability of scoring a 1). The 2PL is a special case of the 3PL, where $\gamma_{i}$ equals $0$ for all items. \\
\indent Combining an IRF with the assumption of conditional independence allows us to model the probability of a score pattern to $k$ items by factoring the probabilities for each item:

\begin{equation}
P(\boldsymbol{X} = \boldsymbol{x} | \theta, \boldsymbol{\nu}) = \prod_{i=1}^{k} \{P(X_i = 1 | \theta, \boldsymbol{\nu})\}^{x_i} \cdot  \{1 - P(X_i = 1 | \theta, \boldsymbol{\nu}) \}^{1 - x_i},
\end{equation}

where $\boldsymbol{X}$ is a random vector indicating a score pattern and $\boldsymbol{x}$ is the realisation of $\boldsymbol{X}$. Note that $\boldsymbol{\nu}$ is a vector containing the item parameters for all $k$ items. By assuming that persons are randomly sampled from a population, the joint marginal probability of a score pattern of an individual becomes:

\begin{equation}
P(\boldsymbol{X} = \boldsymbol{x}) = \int P(\boldsymbol{X} = \boldsymbol{x} | \theta, \boldsymbol{\nu}) \,\phi(\theta)\,d\theta,
\end{equation}

where $\phi(\theta)$ is the univariate density of the latent trait $\theta$. To solve this equation, the density of $\theta$ has to be specified. For example, $\phi(\theta)$ can be specified as a standard Gaussian distribution. With the joint marginal probability and independence of observations, we can construct a likelihood function and estimate $\boldsymbol{\nu}$ through marginal maximum likelihood estimation:

\begin{equation}
\mathcal{L}(\boldsymbol{\nu}) = \prod_{\boldsymbol{x}} (P(\boldsymbol{X} = \boldsymbol{x}))^{n_{\boldsymbol{x}}},
\end{equation}

where $n_{\boldsymbol{x}}$ is the frequency of score pattern $\boldsymbol{x}$. Maximisation of $\mathcal{L}(\boldsymbol{\nu})$ would then lead to the maximum likelihood estimates of the item parameters $\boldsymbol{\hat{\nu}}$. Generally, this is how a model is fit to test data in IRT. \\
\indent After fitting a model, the next step is to test how well the model fits the data. There are multiple options to assess model fit. Typically, both goodness-of-fit tests and fit indices are used. Common goodness-of-fit tests are a $\chi^2$-difference test and Pearson's $\chi^2$ test. The $\chi^2$-difference test uses the following LR statistic:

\begin{equation}
- 2 \ln \left \{ \frac{\max(\mathcal{L}_0)}{\max(\mathcal{L}_a)} \right \},
\end{equation}

where $\mathcal{L}_0$ is the likelihood of the model you fit (i.e., the null model) and $\mathcal{L}_a$ is the likelihood of an alternative model under which the null model is nested. According to Willks' theorem (Wilks, 1938), under the null hypothesis this LR statistic will asymptotically follow a $\chi^2$ distribution as sample size increases. Alternatively, the observed value of Pearson's $\chi^2$ statistic is:

\begin{equation}
\sum_{\boldsymbol{x}} \frac{(n_{\boldsymbol{x}} - \varepsilon_{\boldsymbol{x}})^2}{\varepsilon_{\boldsymbol{x}}},
\end{equation}

where $n_{\boldsymbol{x}}$ is the observed frequency for score pattern $\boldsymbol{x}$ and $\varepsilon_{\boldsymbol{x}}$ is the expected frequency for score pattern $\boldsymbol{x}$ given a model. This $\chi^2$ statistic also asymptotically follows a $\chi^2$ distribution as sample size approaches infinity. The MG LR test that we develop and evaluate in the present study is based on the LR statistic: 

\begin{equation}
-2 \ln \left \{ \frac{\max(\mathcal{L}_0)}{\prod_{j = 1}^g \max(\mathcal{L}_j)} \right \},
\end{equation}
 
where $\mathcal{L}_0$ is the likelihood of the chosen model for the whole dataset and $\mathcal{L}_j$ is the likelihood of the chosen model for group $j$. Group $j$ can be an existing group, or the groups can be gained by randomly assigning observations to $g$ groups. Wilks' theorem also applies to this LR statistic. Because all three aforementioned tests asymptotically follow a $\chi^2$ distribution under the null hypothesis, they can be used to assess goodness-of-fit. \\
\indent For fit indices, we research the performance of the TLI, CFI and ICFI. The TLI and CFI are estimated by comparing the tested model to a baseline and saturated model. Our baseline model is a complete-independence model with the following IRF:

\begin{equation}
P(X_i = 1 | \beta_{i}) = \frac{e^{- \beta_{i}}}{1 + e^{- \beta_{i}}},
\end{equation}

where the probability of scoring a $1$ on item $i$ is dependent only on the difficulty of the item $\beta_i$ and no longer on a latent trait. We argue that this is an appropriate baseline model, because the IRF entails that the joint probability distribution is the product of the marginal probability distributions and the items are independent. In this baseline model, the probability of scoring a $1$ on item $i$ is the proportion of people who score a $1$ on item $i$. From this, it follows that the maximum likelihood estimate of $\beta_{i}$ is:

\begin{equation*}
\hat{\beta_{i}} = \ln(\frac{n - n_i}{n_i}), 
\end{equation*}

where $n_i$ is the number of observations who scored a 1 on item $i$ and $n$ is the total number of observations. In a saturated model there are as many parameters as data points, which leads to a perfect fit. The saturated model is:

\begin{equation}
P(\boldsymbol{X} = \boldsymbol{x}) = \pi_{\boldsymbol{x}},
\end{equation}

where there no longer is an IRF. Instead, perfect fit is gained by allowing each score pattern to have its own parameter $\pi_{\boldsymbol{x}}$. The maximum likelihood estimate of $\pi_{\boldsymbol{x}}$ is then the relative frequency of the score pattern:

\begin{equation*}
\hat{\pi}_{\boldsymbol{x}} = \frac{n_{\boldsymbol{x}}}{n},
\end{equation*}

where $n_{\boldsymbol{x}}$ is the number of observations with score pattern $\boldsymbol{x}$ and $n$ is the total number of observations. With the baseline and saturated model, the TLI and CFI can be calculated through the following formulae:

\begin{equation}
\text{CFI} = 1 - \frac{\text{max}\{(\chi^2_T - df_T), 0\}}{\text{max}\{(\chi^2_T - df_T), (\chi^2_0 - df_0), 0\}},
\end{equation}
\begin{equation}
\text{TLI} = \frac{\chi^2_0/df_0 - \chi^2_T/df_T}{\chi^2_0/df_0 - 1}.
\end{equation}

In both equations, $\chi^{2}_{T}$ is the observed value of the LR statistic of a $\chi^2$-difference test between the tested model and the saturated model with $df_T$ degrees of freedom, and $\chi^{2}_{0}$ is the observed value of the LR statistic of a $\chi^2$-difference test between the baseline model and the saturated model with $df_0$ degrees of freedom. \\
\indent Parallel to the Pearson's $\chi^2$ test, the ICFI gauges model appropriateness by comparing the observed score pattern frequencies to the expected score pattern frequencies given a model according to the formula:

\begin{equation}
\text{ICFI} = \frac{2 \cdot \sum_{\boldsymbol{x}}  n_{\boldsymbol{x}} \cdot \varepsilon_{\boldsymbol{x}}  }{  \sum_{\boldsymbol{x}}  {n_{\boldsymbol{x}}}^2 +  \sum_{\boldsymbol{x}}  {\varepsilon_{\boldsymbol{x}}} ^2 }.
\end{equation} \\

\section{\centering Simulation study I}
In the present simulation study, we consider dichotomous IRT with unidimensional latent traits. For the MG LR test, we investigate its asymptotic properties under the null hypothesis and at which rate the test is able to reject a misspecified model under different conditions when groups are made through random assignment. We compare this rate against the rejection rates of a $\chi^2$-difference and Pearson's $\chi^2$ test. For the TLI, CFI and ICFI, we investigate their mean and variance under correct and incorrect model specification. The simulation study was conducted in R (R Core Team, 2022).
\subsection{Data generation}
Data are generated by first sampling $\theta$ from a standard Gaussian distribution. Then, either the 2PL or 3PL is chosen as basis for the data generation (see \textit{Simulation design}). Item parameters remain static over all conditions. For $\beta_i$, repetitions of the values $[-1.0, -0.5, 0.0, 0.5, 1.0]$ are chosen. As for $\alpha_i$, we choose from the values $[0.7, 0.85, 1.0, 1.15, 1.3]$. To create a more realistic scenario, these two parameters are matched with one another such that for every item difficulty, there are low and high discriminating items. When the data generating model is the 3PL, we set $\gamma_i$ to $0.25$ for every item. Probabilities are then estimated for all items on a test, given $\theta$, the chosen model and its item parameters. By sampling from a binomial distribution with the estimated probabilities, a matrix of simulated responses to a dichotomous test is created.
\subsection{Simulation design}
We decided to vary four factors in this simulation study. Three factors are related to data generation. These factors and their conditions we examine are: three conditions of test length $(I \in{5,10,20})$, five conditions of sample size $(N \in{200, 300, 500, 1000, 1500})$, and two conditions for the model on which data generation is based $(DGM \in{\text{2PL}, \text{3PL}})$. This design results in a total of $30$ conditions. Furthermore, within each of these conditions, we vary the fourth factor: the number of groups the MG LR test is based on $(G \in{2, 3, 4})$. We replicate each simulation condition 300 times. In each replication of each condition, we fit the 2PL. The 2PL is fitted using functions from the \textit{ltm} package (Rizopoulos, 2006), which approximates marginal maximum likelihood estimation through the Gauss-Hermite quadrature rule. Afterwards, we assess the performance of the TLI, CFI and ICFI and the three different types of goodness-of fit tests: (1) a $\chi^2$-difference test, obtained by testing the 2PL under the 3PL with the constraint that all $\gamma_i$ are equal through the \textit{mirt} package (Chalmers, 2012), (2) Pearson's $\chi^2$ test, estimated through aggregating score patterns from the data and comparing the observed score pattern frequencies to the expected score pattern frequencies given the fitted 2PL, and (3) the MG LR test with a varying number of randomly assigned groups. 
\subsection{Performance metrics}
We study performance for the different goodness-of-fit tests by estimating both empirical $\alpha$ and power. Empirical $\alpha$ and power are estimated by obtaining the detection rate, which is the number of times the null hypothesis is rejected divided by the total number of replications per condition. For all goodness-of-fit tests, we chose a level of significance of $\alpha = 0.05$ to reject the null hypothesis. Empirical $\alpha$ is estimated when generating data under the 2PL and fitting the 2PL. Power is estimated when generating data under the 3PL and fitting the 2PL. With these, we compare the different goodness-of-fit tests with one another, where a test with lower power or higher empirical $\alpha$ is seen as performing worse. \\
\indent To measure the performance of the TLI, CFI and ICFI, we estimate their mean and standard error in each condition. Then, we can inspect whether the means of the fit indices decrease in the conditions where data are generated under the 3PL, compared to when data are generated under the 2PL. 

\subsection{Results}
To assess the properties of the MG LR test under the null hypothesis and the performance of the MG LR test with randomised groups, the TLI, CFI and ICFI to detect model misspecification, we conducted the above described simulation study. Non-convergence of the models occurred $<.001$ percent of the time. When test length was $20$, the empirical $\alpha$ and power estimates for Pearson's $\chi^2$ test were not estimated. This is because of the large amount of possible score patterns in these conditions $(2^{20})$, which required too much computational power for the scope of the current study. Furthermore, negative values sometimes occurred for the TLI at higher test lengths under model misspecification. Previous literature has shown that this can occur when the degrees of freedom of the hypothesised model are small and correlations among observed variables are low (Wang \& Wang, 2019; Widaman \& Thompson, 2003). We dealt with this by setting the mean to $0$ if it was negative. \\
% Table for Empirical Alpha results
\begin{table}[ht]
\caption{Empirical $\alpha$ estimates for the different goodness-of-fit tests}
\begin{tabular}{ r r | a c a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{5}{c}{Goodness-of-fit test} \\
\rowcolor{white} \textit{I} & \textit{N} & LR2 & LR3 & LR4 & $\Delta\chi^2$ & P-$\chi^2$ \\
\midrule
5 & 200 & 0.09 & 0.06 & 0.10 & 0.05 & 0.06 \\ 
& 300 & 0.07 & 0.08 & 0.07 & 0.04 & 0.04 \\
& 500 & 0.07 & 0.07 & 0.06 & 0.03 & 0.05 \\
& 1000 & 0.04 & 0.06 & 0.07 & 0.03 & 0.06 \\
& 1500 & 0.07 & 0.07 & 0.06 & 0.02 & 0.04 \\
10 & 200 & 0.08 & 0.11 & 0.11 & 0.03 &  0.22 \\ 
& 300 & 0.04 & 0.05 & 0.09 & 0.04 & 0.20 \\
& 500 & 0.04 & 0.05 & 0.06 & 0.06 & 0.17 \\
& 1000 & 0.02 & 0.06 & 0.06 & 0.03 & 0.12 \\
& 1500 & 0.03 & 0.05 & 0.04 & 0.02 & 0.11 \\
20 & 200 & 0.07 & 0.08 & 0.12 & 0.06 & -- \\ 
& 300 & 0.05 & 0.08 & 0.10 & 0.04 & -- \\
& 500 & 0.07 & 0.08 & 0.06 & 0.03 & -- \\
& 1000 & 0.05 & 0.05 & 0.05 & 0.02 & -- \\
& 1500 & 0.04 & 0.05 & 0.05 & 0.03 & -- \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = two-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; LR2 = MG LR test with $g = 2$; LR3 = MG LR test with $g = 3$; LR4 = MG LR test with $g = 4$; $\Delta\chi^2$ = $\chi^2$-difference test under the three-parameter logistic model with equal $\gamma_i$ constraint; P-$\chi^2$ = Pearson's $\chi^2$ test.
\label{tab:1}
\end{table}

\indent \textit{Table \ref{tab:1}} presents the empirical $\alpha$ estimates for the different $\chi^2$-based goodness-of-fit tests. Here, the asymptotic property of the tests can be observed clearly. As sample size increases, empirical $\alpha$ approximations approach the nominal level of $0.05$. At sample sizes larger than $1000$, the rejection rates of the MG LR and $\chi^2$-difference test do not deviate far from the nominal level for different levels of test lengths. However, this means that for low sample sizes, the LR statistic of the MG LR test does not yet completely follow a $\chi^2$ distribution, resulting in higher rejection rates. For Pearson's $\chi^2$ test, the table shows that as the number of items increases, the empirical $\alpha$ increases greatly from $0.06$ to $0.22$. This is in accordance with the issues we discussed in the introduction, where Pearson's $\chi^2$ test does not follow a $\chi^2$ distribution when many score patterns are low or missing. These results can be seen as confirmation for the theory and design behind our study. Important to remember, however, is that empirical data is almost never as clean as simulated data. \\
% table
\begin{table}[ht]
\caption{Power estimates for the different goodness-of-fit tests}
\begin{tabular}{ r r | a c a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{5}{c}{Goodness-of-fit test} \\
\rowcolor{white} \textit{I} & \textit{N} & LR2 & LR3 & LR4 & $\Delta\chi^2$ & P-$\chi^2$ \\
\midrule
5 & 200 & 0.10 & 0.09 & 0.12 & 0.06 & 0.06 \\ 
& 300 & 0.09 & 0.08 & 0.14 & 0.08 & 0.06 \\
& 500 & 0.07 & 0.09 & 0.12 & 0.11 & 0.04 \\
& 1000 & 0.06 & 0.05 & 0.08 & 0.12 & 0.07 \\
& 1500 & 0.05 & 0.07 & 0.05 & 0.19 & 0.07 \\
10 & 200 & 0.09 & 0.14 & 0.20 & 0.25 & 0.29 \\ 
& 300 & 0.11 & 0.11 & 0.13 & 0.26 & 0.21 \\
& 500 & 0.05 & 0.09 & 0.08 & 0.32 & 0.22 \\
& 1000 & 0.08 & 0.07 & 0.07 & 0.58 & 0.16 \\
& 1500 & 0.06 & 0.06 & 0.06 & 0.75 & 0.22 \\
20 & 200 & 0.10 & 0.11 & 0.16 & 0.52 & -- \\ 
& 300 & 0.08 & 0.10 & 0.11 & 0.66 & -- \\
& 500 & 0.05 & 0.08 & 0.08 & 0.89 & -- \\
& 1000 & 0.03 & 0.05 & 0.06 & 0.97 & -- \\
& 1500 & 0.03 & 0.05 & 0.07 & 1.00 & -- \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = three-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; LR2 = MG LR test with $g = 2$; LR3 = MG LR test with $g = 3$; LR4 = MG LR test with $g = 4$; $\Delta\chi^2$ = $\chi^2$-difference test under the three-parameter logistic model with equal $\gamma_i$ constraint; P-$\chi^2$ = Pearson's $\chi^2$ test.
\label{tab:2}
\end{table}

\indent Next, we present the power estimates of the different goodness-of-fit tests in \textit{Table \ref{tab:2}}. For the $\chi^2$-difference test, the table presents clearly that as sample size and/or test length increase, the power to reject the tested model also increases in favour of the true model up to a power of $1.00$ when $I = 20$ \& $N = 1500$. The results for Pearson's $\chi^2$ test reflect that the test seems to not be able to detecting model misspecification with power estimates only slightly larger than the empirical $\alpha$ estimates. For the MG LR test, the table shows a peculiar result: the power estimates of the test still converge to the nominal level of $0.05$. This entails that when not under the null hypothesis, the LR statistic of the MG LR test still asymptotically follows a $\chi^2$ distribution when groups are based on random assignment. The result indicates that the MG LR test cannot be used as a goodness-of-fit test to detect model misspecification under randomised groups. Moreover, the result also implies that the MG LR test is insensitive to violations of correct model specification in testing for group differences. Therefore, we explore how effective the MG LR test is at detecting group differences and the effect of incorrect model specification on detecting group differences in a follow-up simulation study. \\
% table
\begin{table}[ht!]
\caption{TLI, CFI and ICFI values under correct model specification}
\begin{tabular}{ r r | a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{1}{c}{TLI} & \multicolumn{1}{c}{CFI} & \multicolumn{1}{c}{ICFI} \\
\rowcolor{white} \textit{I} & \textit{N} & \textit{M} (\textit{SE}) & \textit{M} (\textit{SE}) & \textit{M} (\textit{SE}) \\
\midrule
 5 & 200 & 0.63 (0.19) & 0.75 (0.12) & 0.98 (0.01) \\
& 300 & 0.74 (0.14) & 0.82 (0.09) & 0.98 (0.01) \\
& 500 & 0.84 (0.09) & 0.90 (0.06) & 0.99 (0.00) \\
& 1000 & 0.92 (0.05) & 0.94 (0.03) & 0.99 (0.00) \\
& 1500 & 0.95 (0.03) & 0.96 (0.02) & 1.00 (0.00) \\
10 & 200 & 0.06 (0.06) & 0.23 (0.05) & 0.69 (0.04) \\
& 300 & 0.11 (0.05) & 0.27 (0.04) & 0.77 (0.03) \\
& 500 & 0.19 (0.04) & 0.33 (0.04) & 0.85 (0.02) \\
& 1000 & 0.32 (0.03) & 0.45 (0.03) & 0.91 (0.01) \\
& 1500 & 0.42 (0.03) & 0.53 (0.03) & 0.94 (0.01)\\
20 & 200 & 0.04 (0.02) & 0.14 (0.02) & 0.03 (0.01) \\
& 300 & 0.05 (0.02) & 0.14 (0.02) & 0.04 (0.02) \\
& 500 & 0.05 (0.01) & 0.14 (0.01) & 0.06 (0.02) \\
& 1000 & 0.07 (0.01) & 0.16 (0.01) & 0.11 (0.02) \\
& 1500 & 0.08 (0.01) & 0.17 (0.01) & 0.17 (0.03) \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = two-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; TLI = tucker-lewis index; CFI = comparative fit index; ICFI = identity coefficient fit index; \textit{M} = mean; \textit{SE} = standard error.
\label{tab:3}
\end{table}

\indent For the fit indices, \textit{Table \ref{tab:3}} and \textit{Table \ref{tab:4}} exhibit the means and standard errors for the TLI, CFI and ICFI under correct and incorrect model specification respectively. The tables show that as sample size increases, the fit index values move closer to $1$ (i.e., perfect fit). However, as test length increases, fit index values move closer to $0$ instead. This can be interpreted in the following way: as test length increases, the fit of the baseline model becomes relatively better compared to the tested model. Nonetheless, the means for the TLI and CFI are higher under correct model specification compared to incorrect model specification for every single condition (e.g., for $N = 400$ \& $I = 10$, the mean of the TLI is $0.11$ under correct model specification, whereas it is $0.00$ under incorrect model specification). This means that for any test length the TLI and CFI are able to detect model misspecification when comparing multiple models in dichotomous IRT. Furthermore, depending on test length, TLI and CFI values below $0.90$ can still indicate a good fit. However, the tables also show that for cases where there is a small sample size and few items, there are high standard errors for the TLI and CFI. This indicates that when test length and sample size are low, point estimates for the TLI and CFI are not accurate enough to safely decide which model fits best. For this, methods such as bootstrapping should be employed. \\
\begin{table}[ht]
\caption{TLI, CFI and ICFI values under incorrect model specification}
\begin{tabular}{ r r | a c a }
\toprule
\multicolumn{2}{c}{Conditions} & \multicolumn{1}{c}{TLI} & \multicolumn{1}{c}{CFI} & \multicolumn{1}{c}{ICFI} \\
\rowcolor{white} \textit{I} & \textit{N} & \textit{M} (\textit{SE}) & \textit{M} (\textit{SE}) & \textit{M} (\textit{SE}) \\
\midrule
 5 & 200 & 0.33 (0.31) & 0.55 (0.19) & 0.98 (0.01) \\
& 300 & 0.49 (0.24) & 0.66 (0.16) & 0.99 (0.00) \\
& 500 & 0.66 (0.18) & 0.77 (0.12) & 0.99 (0.00) \\
& 1000 & 0.81 (0.11) & 0.87 (0.07) & 1.00 (0.00) \\
& 1500 & 0.86 (0.08) & 0.91 (0.05) & 1.00 (0.00) \\
10 & 200 & 0.00 (0.04) & 0.11 (0.03) & 0.76 (0.05) \\
& 300 & 0.00 (0.04) & 0.13 (0.03) & 0.83 (0.03) \\
& 500 & 0.00 (0.03) & 0.17 (0.03) & 0.89 (0.02) \\
& 1000 & 0.07 (0.03) & 0.24 (0.02) & 0.94 (0.01) \\
& 1500 & 0.15 (0.03) & 0.31 (0.02) & 0.96 (0.01) \\
20 & 200 & 0.00 (0.01) & 0.06 (0.01) & 0.04 (0.02) \\
& 300 & 0.00 (0.01) & 0.06 (0.01) & 0.06 (0.03) \\
& 500 & 0.00 (0.01) & 0.07 (0.01) & 0.10 (0.03) \\
& 1000 & 0.00 (0.01) & 0.07 (0.01) & 0.17 (0.03) \\
& 1500 & 0.00 (0.01) & 0.08 (0.01) & 0.24 (0.03) \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; Data generating model = three-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; TLI = tucker-lewis index; CFI = comparative fit index; ICFI = identity coefficient fit index; \textit{M} = mean; \textit{SE} = standard error.
\label{tab:4}
\end{table}

\indent The ICFI shows less promise compared to the TLI and CFI in its ability to determine model fit in dichotomous IRT. The tables show that when data is generated under the 3PL, the ICFI shows either no difference or a small improvement in the fit of the 2PL compared to when data is generated under the 2PL. Therefore, we argue that the ICFI is unable to determine whether there is model misspecification. This finding corresponds with the results from the Pearson's $\chi^2$ test. Together, they strongly indicate that model fit measures based on observed and expected score-pattern frequencies are not able to detect model misspecification when working with the 2PL. \\

\section{\centering Simulation Study II}
Because of the result we found in Simulation Study I, where under model misspecification the MG LR test asymptotically follows a $\chi^2$ distribution, we were inspired to run a follow-up simulation study. Here, we investigate how effective the MG LR test is at detecting different types of group differences under correct and incorrect model specification for different test lengths and sample sizes. Besides general effectiveness, we are interested in ascertaining whether there is a difference in effectiveness of the MG LR test to detect group differences when an incorrect model is fit to the data compared to the correct model. We only consider IRT with unidimensionality and dichotomous test items. The simulation study was conducted in R. \\
\indent Similar to Simulation Study I, we vary the four factors test length $(I \in{5, 10, 20})$, sample size $(N \in{200, 300, 500, 1000, 1500})$, model type $(\text{DGM} \in{\text{2PL}, \text{3PL}})$ and number of groups $(G \in{2, 3, 4})$. We also investigate a fifth factor: type of group differences, which has three conditions: (1) differences in $\theta$, (2) differences in the item parameters $\alpha_i$ and $\beta_i$ and (3) differences in the item parameters $\alpha_i$, $\beta_i$ and $\gamma_i$ (only applicable to the 3PL condition). This results in a total of $225$ conditions. Each condition is replicated $300$ times. \\
\indent Data generation for Simulation Study II works in the same way as in Simulation Study I with one key difference. Now, data generation is no longer according to the same parameters described in Simulation Study I for the whole dataset. Instead, the parameters are dependent on two factors: the number of groups and the type of group differences. In the differences in $\theta$ conditions, the item parameters are the same for all $g$ groups. However, the mean of the Gaussian distribution for $\theta$ depends on group membership. For each consecutive every group past the first group, there is a mean difference of $-0.25$ with the previous group. For example, when $g = 3$, group one has a mean of 0, group two a mean of $-0.25$, and group three a mean of $-0.50$. In the differences in item parameter conditions, the person parameters are all sampled from the same standard Gaussian distribution. However, the item parameters now depend on group membership. For $\alpha_i$, all initial values (identical to Simulation Study I) are lowered by $0.10$ for each consecutive group. Whereas for $\beta_i$, the values are lowered by $0.25$ per consecutive group. To illustrate, when $g = 2$, group one has $\alpha_1 = 0.70$ and $\beta_1 = -1$, and group two has $\alpha_1 = 0.60$ and $\beta_1 = -1.25$. For the conditions where $\gamma_i$ also differs per group, we lower the $\gamma_i$ values by .05 per consecutive group. We no longer estimate the $\chi^2$-difference and Pearson's $\chi^2$ test, as these are used to detect model misspecification, whereas we are now interested in detecting group differences. Performance is measured by the detection rate, which is considered a power estimates against group differences. \\

\subsection{Results}
To research whether the MG LR test has power to discern whether there are group differences and the effect incorrect model specification has on the power of the test, we ran the above described simulation study where data is generated according to different types of group differences. Non-convergence of the models occurred $<.001$ percent of the time.
\begin{table}[ht]
\caption{Power estimates for the MG LR test under group differences in $\theta$}
\begin{tabular}{ r r | a c a | c a c }
\toprule
\multicolumn{2}{c}{} & \multicolumn{6}{c}{Data Generating Model} \\
\multicolumn{2}{c}{} & \multicolumn{3}{c}{2PL} & \multicolumn{3}{c}{3PL} \\
\multicolumn{2}{c}{Conditions} & \multicolumn{3}{c}{\textit{G}} & \multicolumn{3}{c}{\textit{G}} \\
\rowcolor{white} \textit{I} & \textit{N} & 2 & 3 & 4 & 2 & 3 & 4 \\
\midrule
5 & 200 & 0.13 & 0.28 & 0.37 & 0.15 & 0.24 & 0.31 \\ 
& 300 & 0.15 & 0.32 & 0.46 & 0.14 & 0.23 & 0.41 \\
& 500 & 0.20 & 0.42 & 0.71 & 0.17 & 0.36 & 0.59 \\
& 1000 & 0.40 & 0.79 & 0.98 & 0.33 & 0.64 & 0.90 \\
& 1500 & 0.65 & 0.96 & 1.00 & 0.47 & 0.85 & 0.98 \\
10 & 200 & 0.13 & 0.26 & 0.40 & 0.18 & 0.23 & 0.44 \\ 
& 300 & 0.17 & 0.24 & 0.44 & 0.15 & 0.30 & 0.50 \\
& 500 & 0.18 & 0.43 & 0.72 & 0.19 & 0.34 & 0.63 \\
& 1000 & 0.42 & 0.80 & 0.98 & 0.38 & 0.66 & 0.92 \\
& 1500 & 0.62 & 0.97 & 1.00 & 0.49 & 0.89 & 1.00 \\
20 & 200 & 0.13 & 0.23 & 0.35 & 0.10 & 0.25 & 0.39 \\ 
& 300 & 0.14 & 0.23 & 0.41 & 0.13 & 0.26 & 0.37 \\
& 500 & 0.17 & 0.40 & 0.65 & 0.15 & 0.36 & 0.57 \\
& 1000 & 0.38 & 0.76 & 0.97 & 0.26 & 0.68 & 0.89 \\
& 1500 & 0.57 & 0.94 & 1.00 & 0.48 & 0.88 & 1.00 \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; 2PL = two-parameter logistic model; 3PL = three-parameter logistic model; \textit{I} = test length; \textit{N} = sample size; \textit{G} = number of groups under which the data was generated.
\label{tab:5}
\end{table}

\indent For an average difference of $0.25$ in $\theta$ between each consecutive group, \textit{Table \ref{tab:5}} presents the power of the MG LR test. The table shows that the power to reject the null hypothesis approaches $1.00$ as sample size increases. The table also shows a main effect of the number of groups, where the power to reject the null hypothesis increases as the number of groups increases. The power estimates are slightly lower in conditions when the data is generated under the 3PL. This means that incorrect model specification leads to a small loss in power. Finally, the table shows no main effect of test length on the power of the MG LR test to detect group differences.
% Table 6
\begin{table}[ht!]
\caption{Power estimates for the MG LR test under group differences in item parameters}
\begin{tabular}{ r r | a c a | c a c | a c a }
\toprule
\multicolumn{2}{c}{} & \multicolumn{9}{c}{Data Generating Model} \\
\multicolumn{2}{c}{} & \multicolumn{3}{c}{2PL} & \multicolumn{3}{c}{3PL} & \multicolumn{3}{c}{3PL $( \gamma_i ) $}\\
\multicolumn{2}{c}{Conditions} & \multicolumn{3}{c}{\textit{G}} & \multicolumn{3}{c}{\textit{G}} & \multicolumn{3}{c}{\textit{G}} \\
\rowcolor{white} \textit{I} & \textit{N} & 2 & 3 & 4 & 2 & 3 & 4 & 2 & 3 & 4 \\
\midrule
5 & 200 & 0.12 & 0.31 & 0.36 & 0.15 & 0.24 & 0.30 & 0.19 & 0.46 & 0.57 \\ 
& 300 & 0.20 & 0.34 & 0.43 & 0.14 & 0.23 & 0.39 & 0.28 & 0.51 & 0.80 \\
& 500 & 0.22 & 0.43 & 0.73 & 0.18 & 0.38 & 0.56 & 0.41 & 0.80 & 0.96 \\
& 1000 & 0.46 & 0.82 & 0.98 & 0.33 & 0.68 & 0.86 & 0.76 & 1.00 & 1.00 \\
& 1500 & 0.72 & 0.98 & 1.00 & 0.51 & 0.85 & 0.98 & 0.93 & 1.00 & 1.00 \\
10 & 200 & 0.14 & 0.30 & 0.48 & 0.19 & 0.31 & 0.50 & 0.31 & 0.49 & 0.75 \\ 
& 300 & 0.21 & 0.30 & 0.51 & 0.18 & 0.35 & 0.62 & 0.36 & 0.64 & 0.88 \\
& 500 & 0.22 & 0.45 & 0.80 & 0.21 & 0.39 & 0.65 & 0.49 & 0.85 & 0.99 \\
& 1000 & 0.53 & 0.88 & 0.98 & 0.46 & 0.76 & 0.93 & 0.87 & 1.00 & 1.00 \\
& 1500 & 0.76 & 0.98 & 1.00 & 0.63 & 0.90 & 1.00 & 0.99 & 1.00 & 1.00 \\
20 & 200 & 0.13 & 0.23 & 0.35 & 0.10 & 0.25 & 0.39 & 0.21 & 0.54 & 0.75 \\ 
& 300 & 0.14 & 0.23 & 0.41 & 0.13 & 0.26 & 0.37 & 0.32 & 0.63 & 0.89 \\
& 500 & 0.17 & 0.40 & 0.65 & 0.15 & 0.36 & 0.57 & 0.46 & 0.87 & 1.00 \\
& 1000 & 0.38 & 0.76 & 0.97 & 0.26 & 0.68 & 0.89 & 0.87 & 1.00 & 1.00 \\
& 1500 & 0.57 & 0.94 & 1.00 & 0.48 & 0.88 & 1.00 & 0.97 & 1.00 & 1.00 \\
\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. Fitted model = two-parameter logistic model; 2PL = two-parameter logistic model.; 3PL = three-parameter logistic model where $\alpha_i$ and $\beta_i$ differ per group; 3PL $ (\gamma_i)$: three-parameter logistic model where $\alpha_i$, $\beta_i$ and $\gamma_i$ differ per group; \textit{I} = test length; \textit{N} = sample size; \textit{G} = number of groups under which the data was generated.
\label{tab:6}
\end{table}

\indent \textit{Table \ref{tab:6}} presents the power of the MG LR test when group differences are based on the item parameters. Similar results can be found here, where power increases as either sample size or the number of groups increases. There is again no main effect of test length on power. Incorrect model specification still leads to a slight decrease in power estimates compared to correct model specification. However, looking at the conditions where $\alpha_i$, $\beta_i$ and $\gamma_i$ change per group, we find that the power estimates are much higher compared to the corresponding conditions where solely $\alpha_i$ and $\beta_i$ change per group. This entails that the power of the MG LR test is dependent on the amount of group differences in item parameters, even if these group differences cannot properly be reflected in the fitted model. The overarching results are that the MG LR test seems to be very perceptive to group differences in either the distribution of the latent trait or item parameters, especially with sample sizes larger than $500$ and the number of groups exceeds $2$, and that incorrect model specification leads to only a slight loss in power.\\

\section{\centering Empirical example}
To demonstrate how a $\chi^2$-difference test, the TLI and CFI and theory can be used together to perform model appraisal in dichotomous IRT on empirical data, we estimate and interprete their values on Section 6 of the Law School Admission Test (LSAT) dataset given by Bock \& Lieberman (1970). The dataset contains $1000$ observations on $5$ homogenous Figure Classification items on the unidimensional subject of Debate. The items are multiple choice, where a $1$ denotes a correct answer and a $0$ denotes a wrong answer. We fit four models in total: (1) the 2PL, (2) the 3PL with no constraints, (3) the 3PL with the constraint that all $\gamma_i$ are equal and (4) the 3PL where we set all $\gamma_i$ to $0.25$. Afterwards, we evaluate the appropriateness of each model by calculating the TLI and CFI. Furthermore, we perform two $\chi^2$-difference tests with the 2PL as the null model. The first test has the 3PL with no constraints as the alternative model, and the second test has the 3PL with the constraint that all $\gamma_i$ are equal as the alternative model. \\
\indent For the 2PL, the TLI equaled $0.57$ and the CFI equaled $0.81$. For the 3PL with no constraints, the TLI lowered to $0.15$ and the CFI lowered $0.73$. A $\chi^2$-difference test between these two models showed a non-significant result $(p > 0.999)$. For the 3PL with equal $\gamma_i$, the TLI was $0.53$ and the CFI $0.80$. The $\chi^2$-difference test between the 2PL model and this model also showed a non-significant result $(p > 0.999)$. Finally, for the 3PL where we set all $\gamma_i$ to $0.25$, the TLI and CFI were again $0.56$ and $0.80$ respectively. \\
\indent From these results, we infer the following: allowing each item to have a free $\gamma_i$ does not lead to an improvement of model fit according to the $\chi^2$-difference test. The TLI and CFI show that it leads to a worse fit. However, setting the constraint that all $\gamma_i$ have to be equal or setting them to a set value of $0.25$ does not lead to a worse fit compared to the 2PL according to the TLI and CFI. Yet the $\chi^2$-difference test shows that constraining all $\gamma_i$ to be equal at the cost of one degree of freedom does not lead to a significant improvement of model fit compared to the 2PL. Nonetheless, we have to take into account the fact that the items were multiple choice and in theory do have a baseline probability of being scored correctly. Therefore, we conclude that the most reasonable models are the 3PL with equal $\gamma_i$ or the 3PL with $\gamma_i$ set to $0.25$. \\

\section{\centering Discussion}

The current study investigated existing and new measures of model fit to test for model misspecification and group differences in dichotomous IRT. Through the first simulation study, we observed that the proposed MG LR test requires sample sizes larger than 1000 in order to properly follow a $\chi^2$ distribution. The MG LR test was not able to detect any model misspecification under random group assignment. Rather, under incorrect model specification the MG LR test still asymptotically followed a $\chi^2$ distribution. Similarly, we observed that fit measures that asses goodness-of-fit through estimating the difference between expected and observed score pattern frequencies (i.e., the ICFI and Pearson's $\chi^2$ test) had barely any power to detect model misspecification when fitting the 2PL. \\
\indent Contrary to these results and the results given by Yang (2020), we found that the TLI and CFI were very sensitive to detecting model misspecification in dichotomous IRT. This finding coincides with the results given by Cai et al. (2021), and we believe that it might be due to our shared choice in baseline model. Previous research by Widaman \& Thompson (2003) and Van Laar \& Braeken (2021) has already discussed the importance of baseline models for incremental fit indices. However, against the expectations of Cai et al. (2021), who speculated that the TLI would not depend heavily on model size (i.e., test length), we discovered that the TLI and CFI estimates lower substantially as test length increases. Whether this is the effect of using a different statistic for the calculation of the TLI or the shared choice of baseline remains unknown. Future research could look into the relationship between baseline model, test length and performance of the TLI and CFI to a greater extent. Fortunately, the effect of test length does not affect the fit indices' sensitivity to model misfit. It only poses issues for any rule-of-thumbs associated with the usage of the TLI and CFI. All things considered, the ability of the TLI and CFI to detect model misspecification in dichotomous IRT remains a relevant finding, especially when considering the results given by Nye et al. (2020) who found few fit indices that were sensitive to model misspecification in this setting. We strongly recommend that future studies in IRT incorporate the TLI and CFI in evaluating model appropriateness. \\
\indent Further exploring properties of the MG LR test, a follow-up simulation study showed that the MG LR test is effective at detecting group differences. The test is able to detect small group differences in the population mean - according to Cohen's (2013) standards for effect size in behavioural sciences - above the 0.80 power threshold once sample size rose above 1000 with more than two groups. A unique property of the MG LR test is that it tests only for group differences and mostly ignored model specification, where incorrect model specification leads only to a small loss in power. Furthermore, the fitted model does not have to reflect all group differences properly for the MG LR test to ascertain whether there are group differences. The largest limitation of the MG LR test is that it is unable to determine where these group differences lie. For this, a researcher has to investigate the latent construct and the items of the questionnaire through Measurement Invariance techniques. Nevertheless, we believe that the MG LR test could be a useful addition to the multi-group analysis literature. \\
\indent Naturally, we have to bear in mind the context of the present study. We studied these different measures of model fit and group differences solely in IRT with dichotomous items and unidimensional latent traits. For the MG LR test, this signifies that we do not know whether the test is also sensitive for group differences beyond this setting. Future work could further investigate the asymptotic properties and power of the test for group differences in polytomous and multidimensional IRT and continuous settings such as in SEM. Furthermore, as briefly mentioned before, we worked with simulated data where we made sure that no assumptions were violated. In real-life settings, this may almost never happen and this should be taken into account with usage of the measures reported here. To illustrate this, we showcased an example of how a $\chi^2$-difference test, TLI and CFI can be used in tandem for model appraisal in dichotomous IRT. \\
\indent To summarise, preliminary evidence brought about our belief that the TLI and CFI show promise as valuable aids for model evaluation in dichotomous IRT. On the contrary, the ICFI, Pearson's $\chi^2$ test and the MG LR test are not able properly assess model fit in this setting. However, when researching differences between groups in unidimensional dichotomous IRT, the MG LR test can be used to detect whether these differences exist while disregarding model specification. Model evaluation in IRT remains an issue that requires more light to be shed upon in future research. Hopefully, the conducted research is able to improve model evaluation ever so slightly in the social and behavioural sciences. 

\begin{sm}
All annotated code and results used in the current study can be found online on the Open Science Framework through the following \href{https://osf.io/dtbcr/?view_only=e32e5f8a43124434b5a53a44ff26ad23}{link}.
\end{sm}

\newpage

\nocite{*}
\bibliographystyle{apalike}
\bibliography{reportref}

\end{document}
