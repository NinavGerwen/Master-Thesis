% setting document class according to SAGE guidelines
\documentclass[Royal,sageapa,times,doublespace]{sagej}

\usepackage{moreverb,url}
\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}

% mathematical packages
\usepackage{amsmath}

% table-related packages
\usepackage{multirow,booktabs,setspace,caption}
\usepackage{tikz}

%layout-related packages
\usepackage{indentfirst}

\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\def\volumeyear{2023}


\begin{document}

\runninghead{van Gerwen and Hessen}

\title{Designing and Evaluating a Goodness-of-Fit Test for IRT models}

\author{Nina van Gerwen \affilnum{1} and Dave Hessen\affilnum{2}}

\affiliation{\affilnum{1}Utrecht University, NL \\
\affilnum{2}Utrecht University, NL}

\corrauth{Nina van Gerwen, Utrecht University,
Faculty of Social Sciences, Department of Methodology and Statistics,
Padualaan 14, Utrecht, 3584 CH, NL.}

\email{n.l.vangerwen@uu.nl}

\begin{abstract}
Abstract text.
\end{abstract}

\keywords{IRT, Goodness-of-fit test, fit indices}

\maketitle

\section{Introduction}
This part will contain information about: IRT, fit indices, goodness of fit tests, issues, etc.
\subsection{The present study}
In order to create a goodness-of-fit test to use for the 3PL in IRT and to better understand the possible uses of the CFI and TLI in an IRT setting, the present study answered the following three research questions through simulation studies:
\begin{enumerate}
\item{What sample size is necessary at different test lengths for the Randomisation test to perform well?}
\item{How does the performance of the Randomisation test compare to the performance of a $\chi^2$-difference and Pearson's $\chi^2$-test?}
\item{What is the performance of the TLI and CFI with a complete-independence baseline model in IRT?}
\end{enumerate}

\section{Methods}
\subsection{Statistical background}
Before we share the methodology of the current study, let us first examine a brief summary on the statistical theory associated with our study. In IRT, the goal is to find the model that best describes scores on test items. To achieve this, IRT presupposes three assumptions: (1) conditional independence of items given the latent trait, denoted by $\theta$, (2) independence of observations and (3) the response to an item can be modeled by an item response function (IRF). Note that $\theta$ tends to be unidimensional, however it can be generalised to a multidimensional setting. An IRF is a mathematical equation that calculates the probability to score a certain category on an item given $\theta$. In the present study, we considered IRT with unidimensional $\theta$, dichotomous test items and the following three IRF:

\begin{equation}
P(X_i = 1 | \theta, \beta_{i}) = \frac{e^{\theta - \beta_{i}}}{1 + e^{\theta - \beta_{i}}},
\end{equation}

which is known as the one-parameter logistic model (1PL), where $X_i$ is a random variable indicating the response to item $i$. The probability of scoring a 1 on item $i$ in the 1PL model depends on the latent variable, $\theta$, that you are trying to measure and the difficulty of the item, $\beta_i$.

\begin{equation}
P(X_i = 1 | \theta, \alpha_{i}, \beta_{i}) = \frac{e^{\alpha_{i}\theta + \beta_{i}}}{1 + e^{\alpha_{i}\theta + \beta_{i}}},
\end{equation}

this is a generalisation of the 1PL, known as the two-parameter logistic model (2PL), where the probability of scoring a 1 now also depends on an item-dependent intercept term $\alpha_i$, which shows how well item $i$ discriminates between individuals who score a 0 and individuals who score a 1. This IRF can be generalised even further to the three-parameter logistic model (3PL):

\begin{equation}
P(X_i = 1 | \theta, \alpha_{i}, \beta_{i}, \gamma_{i}) = \gamma_{i} + (1 - \gamma_{i}) \cdot 
\frac{e^{\alpha_{i}\theta + \beta_{i}}}{1 + e^{\alpha_{i}\theta + \beta_{i}}},
\end{equation}

where the probability of scoring a 1 on item $i$ is further dependent on an item-specific lower asymptote $\gamma_i$, which indicates whether there is
a baseline probability of scoring a 1 (e.g., a multiple choice test with 4 options has a .25 baseline probability of scoring a 1).

Then, due to the assumption of conditional independence, we can model the probability of a complete score pattern to $k$ items simply by factoring the probability for each item:

\begin{equation}
P(\boldsymbol{X_a} = \boldsymbol{x_a} | \theta_a, \boldsymbol{\nu}) = \prod_{i=1}^{k} P(X_i = 1 | \theta_a, \boldsymbol{\nu}),
\end{equation}

where $\boldsymbol{X}$ is now a random variable indicating a scorepattern (i.e., a vector of 0's and 1's) and $\boldsymbol{x_a}$ is the realisation of $\boldsymbol{X_a}$ for person $a$. Note that $\boldsymbol{\nu}$ is a vector containing item parameters for all $k$ items. We can take this even further by taking into account the assumption of independence of observations and the assumption that persons are randomly sampled from a population. Then, the joint marginal probability of all score patterns in a given sample will become:

\begin{equation}
\int \prod_{i=1}^{k} \{ P(\boldsymbol{X_a} = \boldsymbol{x_a} | \theta_a, \boldsymbol{\nu}) \} \,d\phi(\theta)\,d\theta,
\end{equation}

where $\phi(\theta)$ is the univariate density of the latent variable $\theta$. In order to solve this equation, the density of $\theta$ has to be specified. In the current study, we assume $\theta$ to always be a standard normal distribution. \\
\indent With the joint marginal probability, we can construct a likelihood function and estimate $\boldsymbol{\nu}$ through marginal maximum likelihood estimation. 

then talk about model fit testing and fit indices

For the calculation of fit indices, two more IRF are considered...

Complete independence:
\begin{equation}
P(X_i = 1 | \beta_{i}) = \frac{e^{\beta_{i}}}{1 + e^{\beta_{i}}},
\end{equation}

In the complete independence model, the probability of scoring a 1 on item $i$ is dependent only on the difficulty of the item and no longer on
a latent variable. This entails that the joint probability distribution is simply the product of the marginal probability distributions and therefore the
items will no longer correlate with one another (i.e., they are independent).

Saturated model:
\begin{equation}
P(X_i = 1) = \frac{n_{X_i = 1}}{N},
\end{equation}
\subsubsection{Fit indices} 
The current study investigated the CFI and TLI, which can be calculated through the following formulae: \\

CFI:
\begin{equation}
CFI = 1 - \frac{\chi^{2} - df}{\chi^{2}_{0} - df_0}
\end{equation}
TLI:
\begin{equation}
TLI = 1 - \frac{\chi^{2}/df}{\chi^{2}_{0}/df_0}
\end{equation}
where the numerator is a $\chi^2$-difference test between the tested model and the saturated model with $df$ degrees of freedom 
and the denominator is a $\chi^2$-difference test between the tested model and the complete independence model with $df_0$ degrees of freedom.

\subsubsection{Goodness-of-fit tests}

We compared the performance of the following three goodness-of-fit tests.
\begin{itemize}
\item{$\chi^2$-difference test}
	\begin{itemize}
	\item{The 1PL model was tested under the 2PL model. The 2PL model was tested under the 3PL model. For the 3PL model, this test was not used due to the limitations named in the Introduction.}
	\end{itemize}
\item{Pearson's $\chi^2$-test}
	\begin{itemize}
	\item{Calculated through observing the differences in the observed and expected frequency of score patterns: $\sum_{i = 1}^{n}\frac{(Obs_i - Exp_i)^2}{Exp_i}$}
	\end{itemize}
\item{LR randomisation test}
	\begin{itemize}
	\item{This is the test that we have developed and tested in the current study. The formula of the test statistic is:
	\begin{equation}
		\frac{max(L_0)}{\prod_{j = 1}^g max(L_j)}
	\end{equation} where $L_0$ is the likelihood of the chosen model for the whole dataset and $L_j$ is the likelihood of the chosen model for each group, gained by randomly assigning the observations to $g$ grouops. According to Wilk's theorem \cite{willkth}, this LR will then asymptotically follow a $\chi^2$-distribution. This allows the Randomisation test to be used for Null Hypothesis Significance testing.} 
	\end{itemize}
\end{itemize}

\subsection{Data generation}
Data generation was done by first sampling person parameters, $\theta$, from a standard normal distribution. Then, a model was chosen as basis for the data generation. We chose to keep item parameters static over all simulations. For the difficulty parameter $\beta$, we chose the values -2, -1, 0, 1 and 2 for every repetition of five items. As for the discrimination parameter $\alpha$, we chose repetitions of the values 0.7, 0.85, 1, 1.15 and 1.3 per five items. Finally, for the pseudo-guessing parameter $\gamma$, we chose XXX. Then, probabilities were calculated for all items on a test, given $\theta$ and the chosen model's item parameters. Finally, a matrix of 0's and 1's was created by sampling from a binomial distribution for every item, given each person. We replicated each simulation condition (see below) 500 times. Each replication, the item parameters remained the same and only new person parameters were sampled.

\subsection{Simulation design}
In order to answer the research questions, we conducted a simulation study that varied four factors: test length, sample size, model types and number of groups. For an overview of the conditions we used for the factors, see \textit{Table \ref{tab:1}}. This resulted in a total of 3 (test length) x 5 (sample size) x 3 (model type) x 3 (number of groups) = 135 conditions. \\
\indent In each replication of each condition, we calculated the three goodness-of-fit tests and two fit indices. Performance of the three tests was then studied by estimating both type I error and power. Power was estimated when fitting and testing a different model to the data than the model used to generate the data. Type I error was estimated when fitting and testing the model that was used to generate the data. With these values, we compared the different type of tests with one another, where a test with lower power or higher type I error was noted as performing worse.
To measure the performance of the TLI and CFI, we calculated the proportion of times that the fit indices improved when the correct model was used compared to another model.

\begin{table}[htpb]
\caption{Overview of Simulation Conditions for Each Factor}
\begin{tabular}{ c c c }
\toprule
Factor & Conditions & Description \\
 \\
\midrule
\multicolumn{1}{l}{Test length} & 5 - 10 - 20 & \multicolumn{1}{l}{\shortstack{The total number of items that the test \\ will consist of}} \\ \\ 
\multicolumn{1}{l}{Sample size} & 20 - 50 - 100 - 200 - 500 & \multicolumn{1}{l}{\shortstack{The total number of observations that \\ will be available for each item}} \\ \\
\multicolumn{1}{l}{Model type} & 1PL - 2PL - 3PL & \multicolumn{1}{l}{\shortstack{The models that we will use as the basis for \\ both data generation and model-fitting}} \\ \\
\multicolumn{1}{l}{Number of groups} & 2 - 3 - 4 & \multicolumn{1}{l}{\shortstack{The number of groups that the total dataset \\ gets divided into for the \\ Randomisation test calculations}} \\

\bottomrule
\end{tabular}

\bigskip
\small\textit{Note}. 1PL = one-parameter logistic model; 2PL = two-parameter logistic model; 3PL = \\ three-parameter logistic model.
\label{tab:1}
\end{table}

\newpage

\section{Results}

\subsection{Empirical example}
\section{Discussion}

\nocite{*}
\bibliographystyle{apalike}
\bibliography{reportref}

\end{document}
